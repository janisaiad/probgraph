\section{Bridging two Gaussian mixture densities}
\label{app:Gauss:mixt}
In this appendix, we consider the case where $\rho_0$ and $\rho_1$ are both Gaussian mixture densities. We denote by
\begin{equation}
\label{eq:NmC}
\begin{aligned}
    {\sf N}(x|m,C) &= (2\pi)^{-d/2} [\det C]^{-1/2} \exp\left(-\tfrac12 (x-m)^\T C^{-1} (x-m)\right),\\
    & = (2\pi)^{-d} \int_{\RR^d} e^{ik\cdot (x-m) -\frac12 k^\T C k} dk,
\end{aligned}
\end{equation}
the Gaussian probability density with mean vector $m \in \RR^d$ and positive-definite symmetric covariance matrix $C=C^\T\in \RR^{d\times d}$. We assume that
\begin{equation}
    \label{eq:rho0:mixt}
    \rho_0(x) = \sum_{i=1}^{N_0} p^0_i {\sf N}(x|m^0_i,C^0_i), \qquad
    \rho_1(x) = \sum_{i=1}^{N_1} p^1_i {\sf N}(x|m^1_i,C^1_i)
\end{equation}
where $N_0,N_1\in \NN$,  $p^0_i>0$ with $\sum_{i=1}^{N_0} p_i^0 = 1$, $m_i^0\in \RR^d$, $C_i^0= (C_i^0)^\T \in \RR^{d\times d}$ positive-definite, and similarly for $p_i^1$, $m_i^1$, and $C_i^1$. 
We have:
\begin{restatable}{proposition}{gaussmixt}
\label{th:Gauss:mixt}
Consider the process $x_t$ defined in~\eqref{eq:stochinterp} using the probability densities in~\eqref{eq:rho0:mixt} and the interpolant in~\eqref{eq:lin:interp}, i.e.
\begin{equation}
    \tag{\ref{eq:lin:interp}}
    x^\LIN_t = \alpha(t) x_0+ \beta(t) x_1 + \gamma(t) z,
\end{equation} 
where $x_0,\sim\rho_0$, $x_1\sim \rho_1$, and $z \sim {\sf N}(0,\Id)$ with $x_0\perp x_1\perp z$, and $\alpha, \beta, \gamma^2\in C^2([0,1])$ satisfy the conditions in \eqref{eq:lin:interp:a:b}. Denote \begin{equation}
    \label{eq:mij:Cij}
    m_{ij}(t) = \alpha(t) m^0_i+\beta(t) m^1_j, \quad C_{ij}(t) = \alpha^2(t) C^0_i+\beta^2(t) C^1_j +\gamma^2(t) \text{\it Id}, 
\end{equation}
where $i=1,\ldots, N_0,$ $j=1,\ldots, N_1$.
Then the probability density $\rho$ of $x_t$ is the Gaussian mixture density
\begin{equation}
    \label{eq:rhot:Gaussmixt}
    \rho(t,x) = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j {\sf N}(x| m_{ij}(t),C_{ij}(t))
\end{equation}
and  the velocity $b$ and the score $s$ defined in~\eqref{eq:b:ode:def} and \eqref{eq:s:def} are 
\begin{equation}
    \label{eq:vt:Gausmixt}
    b(t,x) = \frac{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j \left(\dot m_{ij}(t) + \tfrac12\dot C_{ij}(t) C^{-1}_{ij}(t)(x-m^{ij}(t))  \right) {\sf N}(x| m_{ij}(t),C_{ij}(t))}{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j {\sf N}(x| m_{ij}(t),C_{ij}(t))},
\end{equation}
and
\begin{equation}
    \label{eq:st:Gausmixt}
    s(t,x) = -\frac{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  C^{-1}_{ij}(t)(x-m_{ij}(t))  {\sf N}(x| m_{ij}(t),C_{ij}(t))}{\sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_i p^1_j {\sf N}(x| m_{ij}(t),C_{ij}(t))}.
\end{equation}
\end{restatable}
This proposition implies that $b$ and $s$ grow at most linearly in $x$, and are approximately linear in regions where the modes of $\rho(t,x)$ remain well-separated. In particular, if $\rho_0$ and $\rho_1$ are both Gaussian densities, $\rho_0=\mathsf{N}(m_0,C_0)$ and $\rho_1=\mathsf{N}(m_1,C_1)$, we have  
\begin{equation}
    \label{eq:vt:mode:Gaussmixt}
    b(t,x)=\dot m(t) + \tfrac12 \dot C(t) C^{-1}(t)(x-m(t)),
\end{equation}
and
\begin{equation}
    \label{eq:st:mode:Gaussmixt}
    s(t,x)= - C^{-1}(t)(x-m(t))  ,
\end{equation}
where
\begin{equation}
    \label{eq:mt:Ct}
    m(t) = \alpha(t) m_0+ \beta(t) m_1, \quad C(t) = \alpha^2(t) C_0+ \beta^2(t) C_1 +\gamma^2(t) \text{\it Id}.
\end{equation}
Note that the probability flow ODE~\eqref{eq:ode:1} associated with the velocity~\eqref{eq:vt:mode:Gaussmixt} is the linear ODE
\begin{equation}
    \label{eq:ode:gm}
    \frac{d}{dt} X_t = \dot m(t) + \tfrac12 \dot C(t) C^{-1}(t)(X_t-m(t)).
\end{equation}
This equation can only be solved analytically if $\dot C(t)$ and $C(t)$ commute (which is the case e.g. if $C_0 = \Id$), but it is easy to see that it always guarantees that 
\begin{equation}
    \label{eq:ode:gm:sol}
    \EE_0 X_t(x_0) =  m(t), \qquad \EE_0 \big[(X_t(x_0)-m(t)) (X_t(x_0)-m(t))^\T\big]  = C(t),
\end{equation}
where $X_t(x_0)$ denotes the solution to~\eqref{eq:ode:gm} for the initial condition $X_{t=0}(x_0) = x_0$ and $\EE_0$ denotes expectation over $x_0\sim \rho_0$. A similar statement is true if we solve \eqref{eq:ode:gm} with final conditions at $t=1$ drawn from $\rho_1$. Similarly, the forward SDE~\eqref{eq:sde:1} associated with the velocity~\eqref{eq:vt:mode:Gaussmixt}  and the score~\eqref{eq:st:mode:Gaussmixt} is the linear SDE
\begin{equation}
    \label{eq:sde:gm}
    d X^\fwd_t = \dot m(t)dt + \big(\tfrac12 \dot C(t) - \eps\big) C^{-1}(t)(X^\fwd_t-m(t)) dt + \sqrt{2\eps} dW_t.
\end{equation}
and its solutions are such that  
\begin{equation}
    \label{eq:sde:gm:sol}
    \EE_0 \EE^{x_0}_\fwd X_t^\fwd =  m(t), \qquad \EE_0 \EE^{x_0}_\fwd\big[(X^\fwd_t-m(t) )(X_t^\fwd-m(t))^\T\big]  = C(t),
\end{equation}
where $\EE_\fwd^{x_0}$ denotes expectation over the solution of \eqref{eq:sde:gm} conditional on the event $X^\fwd_{t=0}=x_0$ and $\EE_0$ denotes expectation over $x_0\sim \rho_0$. A similar statement also holds for the backward SDE~\eqref{eq:sde:R}.

\begin{proof} The characteristic function of $\rho(t,x)$ is given by
\begin{equation}
    \label{eq:rhot:Gaussmixt:k}
    g(t,k)  = \EE e^{ik\cdot x_t} = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  e^{ik\cdot m_{ij}(t) - \tfrac12 k^\T C_{ij}(t) k } .
\end{equation}
whose inverse Fourier transform is~\eqref{eq:rhot:Gaussmixt}. This automatically implies~\eqref{eq:st:Gausmixt} since we know from~\ref{eq:s:def} that  $s= \nabla \log \rho$. To derive \eqref{eq:vt:mode:Gaussmixt} use the function $m$ defined below in~\eqref{eq:F12k}:
\begin{equation}
    \label{eq:m:Gaussmixt:k}
    m(t,k)  = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  (\dot m_{ij}(t) +\tfrac12 i \dot C_{ij}(t) k)e^{ik\cdot m_{ij}(t) - \tfrac12 k^\T C_{ij}^\gamma(t) k } .
\end{equation}
From~\eqref{eq:F1k:c}, we know that the inverse Fourier transform of this function is $b\rho$, so that we obtain
\begin{equation}
    \label{eq:m:Gaussmixt:x}
    b(t,x) \rho(t,x) = \sum_{i=1}^{N_0}\sum_{j=1}^{N_1}
    p^0_ip^1_j  \left(\dot m_{ij}(t) + \tfrac12\dot C_{ij}(t) C_{ij}^{-1}(t)(x-m_{ij}(t))\right) {\sf N}(x| m_{ij}(t),C^\eps_{ij}(t)).
\end{equation}
This gives~\eqref{eq:vt:Gausmixt}.
\end{proof}


\section{Proofs}
In this appendix, we provide the details for proofs omitted from the main text. For ease of reading, a copy of the original theorem statement is provided with the proof.


\subsection{Proof of Theorems~\ref{prop:interpolate}, ~\ref{prop:interpolate_losses}, and~\ref{thm:score}, and Corollary~\ref{prop:interpolate_fpe}.}
\label{app:proof:interpolate}
\interpolation*
\begin{proof}
 Let $g(t,k) = \EE e^{ik \cdot x_t}$,  $k \in \RR^d $, be the characteristic function of $\rho(t,x)$. From the definition of $x_t$ in~\eqref{eq:stochinterp},
\begin{equation}
\label{eq:charact}
 g(t,k) = \EE e^{ik\cdot (I(t,x_0,x_1) + \gamma(t) z)}.
\end{equation}
 Using the independence between $(x_0,x_1)$ and $z$ , we have
\begin{equation}
    \label{eq:gt:k}
    g(t,k) = \EE \left(e^{ik\cdot I(t,x_0,x_1) }\right) \EE \left( e^{i\gamma(t)  k\cdot z}\right) \equiv g_0(t,k)  e^{-\tfrac12 \gamma^2(t) |k|^2 }
\end{equation}
where we defined
\begin{equation}
    \label{eq:G0}
    g_0(t,k) = \EE \left(e^{ik\cdot I(t,x_0,x_1) }\right)
\end{equation}
The function~$g_0(t,k) $ is the characteristic function of $I(t,x_0,x_1)$ with $(x_0,x_1)\sim\nu$. From~\eqref{eq:gt:k}, we have
\begin{equation}
    \label{eq:bound:g}
    |g(t,k)| = |g_0(t,k)|  e^{-\tfrac12 \gamma^2(t) |k|^2  } \le e^{-\tfrac12 \gamma^2(t) |k|^2  }
\end{equation} 
Since $\gamma(t)>0$ for all $t\in(0,1)$ by assumption, this shows that 
\begin{equation}
    \label{eq:int:g}
    \forall p \in \NN\ \ \text{and} \ \ t\in(0,1) \quad : \quad \int_{\RR^d} |k|^p |g(t,k)|dk<\infty,
\end{equation} 
implying that $\rho(t,\cdot)$ is in $C^p(\RR^d)$ for any $p\in \NN$ and all $t\in (0,1)$.  
From~\eqref{eq:gt:k}, we also have
\begin{equation}
    \label{eq:bound:dtg}
    \begin{aligned}
        |\partial_t g(t,k)|^2 & = \left|\EE[ (ik\cdot \partial_t I_t(x_0,x_1) - \gamma(t) \dot \gamma(t) |k|^2)  e^{ik\cdot I_t(x_0,x_1)}]\right|^2 e^{-\gamma^2(t) |k|^2 }  \\ 
        &\le 2\left( |k|^2 \EE\big[|\partial_t I_t(x_0,x_1)|^2] + |\gamma(t) \dot \gamma(t)|^2 |k|^4\right) e^{-\gamma^2(t) |k|^2 }\\
        &\le 2\left( |k|^2 M_1 + 4|\gamma(t) \dot \gamma(t)|^2 |k|^4\right) e^{-\gamma^2(t)  |k|^2 }
    \end{aligned}
\end{equation}
and 
\begin{equation}
    \label{eq:bound:dttg}
    \begin{aligned}
        |\partial^2_t g(t,k)|^2 & \le 4\left( |k|^2 \EE\big[|\partial^2_t I_t(x_0,x_1)|^2] +(|\dot \gamma(t)|^2 + \gamma(t) \ddot \gamma(t))^2 |k|^4\right) e^{-\gamma^2(t) |k|^2 }\\
        & \quad + 8\left( |k|^2 \EE\big[|\partial_t I_t(x_0,x_1)|^4] +  (\gamma(t) \dot \gamma(t))^4  |k|^8\right) e^{-\gamma^2(t)  |k|^2 }  \\ 
        &\le 4\left( |k|^2 M_2 +(|\dot \gamma(t)|^2 + \gamma(t) \ddot \gamma(t))^2 |k|^4\right) e^{-\gamma^2(t) |k|^2 }\\
        & \quad + 8\left( |k|^2 M_1 +  (\gamma(t) \dot \gamma(t))^4  |k|^8\right) e^{-\gamma^2(t)  |k|^2 } 
    \end{aligned}
\end{equation}
where in both cases we used~\eqref{eq:It:L2} in Assumption~\ref{as:rho:I} to get the last inequalities. These imply that
\begin{equation}
    \label{eq:int:dg}
    \forall p \in \NN\ \ \text{and} \ \ t\in(0,1) \quad : \quad \int_{\RR^d} |k|^p |\partial_t g(t,k)|dk<\infty; \quad \int_{\RR^d} |k|^p |\partial^2_t g(t,k)|dk<\infty
\end{equation} 
indicating that $\partial_t \rho(t,\cdot)$ and $\partial^2_t \rho(t,\cdot)$ are in $C^p(\RR^d)$ for any $p\in \NN$, i.e. $\rho\in C^1((0,1); C^p(\RR^d))$ as claimed. To show that $\rho$ is also positive, denote by $\mu_0(t,dx)$ the unique (by the Fourier inversion theorem) probability measure associated with $g_0(t,k)$, i.e. the measure such that
\begin{equation}
    \label{eq:meas}
    g_0(t,k) = \int_{\RR^d} e^{ik\cdot x} \mu_0(t,dx).
\end{equation}
From~\eqref{eq:gt:k} and the convolution theorem it follows that we can express $\rho$ as
\begin{equation}
    \label{eq:rhot:invF}
    \rho(t,x) = \int_{\RR^d } \frac{e^{-|x-y|^2/(2\gamma^2(t))}}{(2\pi \gamma^2(t))^{d/2}} \mu_0(t,dy),
\end{equation}
This shows that  $\rho>0$ for all $(t,x)\in (0,1)\times \RR^d$. Since $x_{t=0} = x_0$ and  $x_{t=1} = x_1$ by definition of the interpolant, we also have $\rho(0) = \rho_0$ and $\rho(1)=\rho_1$, which shows that $\rho$ is also positive and in $C^p(\RR^d)$  at $t=0,1$ by Assumption~\ref{as:rho:I}. Note that since $\rho \in C^1((0,1); C^p(\RR^d))$ and is positive, we also immediately deduce that $s = \nabla \log \rho = \nabla \rho /\rho \in C^1((0,1); (C^p(\RR^d))^d)$.

To show that $\rho$ satisfies the TE~\eqref{eq:transport}, we take the time derivative of~\eqref{eq:charact} to deduce that
\begin{equation}
    \label{eq:ito2}
    \partial_t g(t,k) = ik\cdot m(t,k)
\end{equation}
where $m: [0,1]\times \RR^d\to\CC^d$ is the vector-valued function defined as
\begin{equation}
    \label{eq:F12k}
     m(t,k) = \EE\left(( \partial_t I(t, x_0,x_1) +\dot \gamma(t) z)e^{ik\cdot x_t} \right).
\end{equation}
By definition of the conditional expectation, $m(t,k)$ can be expressed as
\begin{equation}
    \label{eq:F1k:c}
    \begin{aligned}
     m(t,k) & = \int_{\RR^d} \EE\left(( \partial_t I(t, x_0,x_1) +\dot \gamma(t) z) e^{ik\cdot x_t} |x_t=x \right) \rho(t,x) dx\\
     & = \int_{\RR^d} e^{ik\cdot x} \EE\left(( \partial_t I(t, x_0,x_1) +\dot \gamma(t) z) |x_t=x \right) \rho(t,x) dx\\
     & = \int_{\RR^d} e^{ik\cdot x} b(t,x) \rho(t,x) dx
     \end{aligned}
\end{equation}
where the last equality follows from the definition of $b$ in~\eqref{eq:b:ode:def}. Inserting~\eqref{eq:F1k:c} in~\eqref{eq:ito2}, we deduce that this equation can be written in real space as the TE~\eqref{eq:transport}.



Let us now investigate the regularity of $b$. To that end, we go back to $m$ and use the independence  between $x_0$, $x_1$, and $z$, as well as Gaussian integration by parts to deduce that
\begin{equation}
    \label{eq:F1:1}
    m(t,k) = \EE\left((\partial_t I(t,x_0,x_1) -i \gamma(t) \dot\gamma(t) k) e^{ik\cdot I(t,x_1,x_0)}\right) e^{-\tfrac12 \gamma^2(t) |k|^2 },
\end{equation}
As a result
\begin{equation}
    \label{eq:F1:int}
    \begin{aligned}
        |m(t,k)|^2 &= \left|\EE\left((\partial_t I(t,x_0,x_1) -i \gamma(t) \dot\gamma(t) k) e^{ik\cdot I(t,x_1,x_0)}\right)\right|^2 e^{-\gamma^2(t)  |k|^2 } \\
        & \le  2\left(\EE\big[|\partial_t I(t,x_0,x_1)|^2\big] + |\gamma(t) \dot \gamma(t)|^2 |k|^2\right) e^{-\gamma^2(t)  |k|^2 } \\
        & \le 2M_1 e^{-\gamma^2(t)  |k|^2 },
    \end{aligned}
\end{equation}
and
\begin{equation}
    \label{eq:F1:dint}
    \begin{aligned}
        |\partial_t m(t,k)|^2
        &\le 4\left(\EE\big[|\partial^2_t I(t,x_0,x_1)|^2 + (\gamma(t) \ddot \gamma(t) + \dot \gamma^2(t))^2 \right) e^{-\gamma^2(t) |k|^2 } \\
        & \quad + 8|k|^2 \left( \EE \big |\partial_t I(t,x_0,x_1)|^4\big] + (\gamma(t) \dot \gamma(t))^4 |k|^4 \right) e^{-\gamma^2(t) |k|^2 } \\
        & \le 4\left(M_1 + (\gamma(t) \ddot \gamma(t) + \dot \gamma^2(t))^2 \right) e^{-\gamma^2(t) |k|^2 } \\
        & \quad + 8|k|^2 \left( M_2 + (\gamma(t) \dot \gamma(t))^4 |k|^4 \right) e^{-\gamma^2(t) |k|^2 } ,
    \end{aligned}
\end{equation}
where in both cases the last inequalities follow from~\eqref{eq:It:L2}. Therefore 
\begin{equation}
    \label{eq:int:f}
    \forall p \in \NN \ \ \text{and} \ \ t\in(0,1) \quad : \quad \int_{\RR^d} |k|^p |m(t,k)|dk<\infty, \quad \int_{\RR^d} |k|^p |\partial_t m(t,k)|dk<\infty,
\end{equation} 
which implies that the inverse Fourier transform of $m$ is a function $j : [0,1]\times \RR^d \to \RR^d$ that satisfies $j(t, \cdot) \in (C^p(\RR^d))^d$ for any $p\in \NN$ and any $t\in(0, 1)$ and can be expressed as
\begin{equation}
    \label{eq:j1}
    j(t,x) = (2\pi)^{-d} \int_{\RR^d} e^{-ik\cdot x} m(t,k) dk = \EE\left(\partial_t I(t,x_0,x_1)+ \dot \gamma (t) z |x_t = x\right) \rho(t,x) \equiv b(t,x) \rho_t(x)
\end{equation}
where the last equality follows from the definition of $b$ in~\eqref{eq:b:ode:def}. We deduce that $b\in C^0([0,1];(C^p(\RR^d))^d)$ for any $p\in \NN$ since $j\in C^0([0,1];(C^p(\RR^d))^d)$ and $\rho\in C^1([0,1];C^p(\RR^d))$, and $\rho>0$. 


Finally, let us establish~\eqref{eq:bt:bounded}. By~\eqref{eq:It:L2} we have
\begin{equation}
    \label{eq:v:L2}
    \begin{aligned}
        \int_{\RR^d} |b(t,x)|^2 \rho(t,x) dx & = \int_{\RR^d}|\EE\left(\partial_t I(t,x_0,x_1)+\dot \gamma(t) z|x_t = x\right)|^2 \rho(t,x)dx \\
        &\le 2 \int_{\RR^d} \EE\left(|\partial_t I(t,x_0,x_1)|^2+|\dot \gamma(t)|^2 |z|^2|x_t = x\right) \rho(t,x)dx\\
        &  \le 2 \EE\big[|\partial_t I(t,x_0,x_1)|^2 +|\dot \gamma(t)|^2|z|^2\big]\\
        & < 2M_1^{1/2} + 2 d |\dot \gamma(t)|^2,
    \end{aligned} 
\end{equation} 
so that this integral is bounded for all $t\in (0,1)$. To analyze its behavior at the end points, notice that the decomposition \eqref{eq:b:decomp} implies that 
\begin{equation}
\label{eq:b:lim:o=0:1}
\begin{aligned}
    b_0(x) &\equiv \lim_{t\to0} b(t,x) = \EE_1 [\partial_t I(0,x,x_1) ] - \lim_{t\to0} \dot \gamma(t) \gamma(t) s_0(x), \\
    b_1(x) & \equiv \lim_{t\to1} b(t,x) = \EE_0 [\partial_t I(0,x_0,x) ] - \lim_{t\to1} \dot \gamma(t) \gamma(t) s_1(x), 
\end{aligned}
\end{equation}
where $s_0= \nabla \log\rho_0$, $s_1= \nabla \log\rho_1$, $\EE_0$ and $\EE_1$ denote expectations over $x_0\sim\rho_0$ and $x_1\sim \rho_1$, respectively, and we used the property that $x_{t=0}=x_0$ and $x_{t=1} = x_1$. Since $\lim_{t\to0,1} \dot \gamma(t) \gamma(t)$ exists by our assumption that $\gamma^2\in C^1([0,1])$, $b_0$ and $b_1$ are well defined, and 
\begin{equation}
    \label{eq:b0:1:int}
    \int_{\RR^d} |b_0(x)|^2 \rho_0(x) dx < \infty, \qquad \int_{\RR^d} |b_1(x)|^2 \rho_1(x) dx < \infty.
\end{equation}
by Assumption~\ref{as:rho:I}. As a result, the integral in~\eqref{eq:v:L2} is continuous at $t=0$ and $t=1$, so it must be integrable on $[0,1]$, and~\eqref{eq:bt:bounded} holds. 
\end{proof}


\interpolatelosses*
\begin{proof}
    By definition of $\rho$, the objective $\mathcal{L}_b$ defined in~\eqref{eq:obj:v} can also be written as 
\begin{equation}
    \label{eq:EL:v}
    \begin{aligned}
    \mathcal{L}_b[\hat b] &= \int_0^1\int_{\RR^d} \left( \tfrac12|\hat b(t,x)|^2 - \EE\left((\partial_t I(t,x_0,x_1)+\dot\gamma(t) z|x_t=x\right)\cdot \hat b(t,x)\right) \rho(t,x) dxdt\\
    &= \int_0^1\int_{\RR^d} \left( \tfrac12 |\hat b(t,x)|^2 - b(t,x)\cdot \hat b(t,x) \right) \rho(t,x) dxdt
    \end{aligned}
\end{equation}
where we used the definition of $b$ in~\eqref{eq:b:ode:def}. This quadratic objective is bounded from below since
\begin{equation}
    \label{eq:EL:v:b}
    \begin{aligned}
    \mathcal{L}_b[\hat b] &= \tfrac12 \int_0^1\int_{\RR^d} \left|\hat b(t,x) - b(t,x) \right|^2 \rho(t,x) dx dt- \tfrac12  \int_0^1\int_{\RR^d} \left|b(t,x) \right|^2 \rho(t,x) dxdt\\
    & \ge - \tfrac12 \int_0^1 \int_{\RR^d} \left|b(t,x) \right|^2 \rho(t,x) dxdt >-\infty
    \end{aligned}
\end{equation}
where the last inequality follows from~\eqref{eq:bt:bounded}. Since $\rho_t$ is positive the  minimizer of~\eqref{eq:EL:v} is unique and given by $\hat b= b$. 

\end{proof}

\score*
\begin{proof}
Since $\rho \in C^1((0,1); C^p(\RR^d))$ and is positive by Theorem~\ref{prop:interpolate}, we already  know that $s = \nabla \log \rho = \nabla \rho /\rho \in C^1((0,1); (C^p(\RR^d))^d)$.
To establish~\eqref{eq:s:def}, note that, for $t\in(0,1)$ where $\gamma(t)>0$, we have
 \begin{equation}
 \label{eq:gbp}
 \EE\left(z  e^{i\gamma(t) k \cdot z }\right) = -\gamma^{-1}(t)(i\partial_k)  \EE e^{i\gamma(t) k \cdot z } = -\gamma^{-1}(t)(i\partial_k)  e^{-\tfrac12 \gamma^2(t) |k|^2} = i \gamma(t)k e^{-\tfrac12 \gamma^2(t) |k|^2}.
 \end{equation}
 As a result, using the independence between $x_0$, $x_1$, and $z$, we have
\begin{equation}
 \label{eq:gbp:2}
 \EE\left(z  e^{i k \cdot x_t }\right) = i \gamma(t)k g(t,k)
 \end{equation}
 where $g$ is the characteristic function of $x_t$ defined in~\eqref{eq:charact}. Using the properties of the conditional expectation, the left-hand side of this equation  can be written as
\begin{equation}
 \label{eq:gbp:3}
 \EE\left(z  e^{i k \cdot x_t }\right) = \int_{\RR^d} \EE\left(z  e^{i k \cdot x_t }|x_t=x\right) \rho(t,x) dx = \int_{\RR^d} \EE\left(z  |x_t=x\right) e^{i k x_t } \rho(t,x) dx 
 \end{equation}
 Since the left hand side of~\eqref{eq:gbp:2} is the Fourier transform of $-\gamma(t) \nabla \rho(t,x)$, we deduce that
 \begin{equation}
 \label{eq:gbp:4}
 \EE\left(z  |x_t=x\right)  \rho(t,x) = -\gamma(t) \nabla \rho(t,x) = -\gamma(t) s(t,x) \rho(t,x).
 \end{equation}
 Since $\rho(t,x)>0$, this implies~\eqref{eq:s:def} for $t\in(0,1)$ where $\gamma(t)>0$. 


To establish~\eqref{eq:st:bounded}, notice that
\begin{equation}
    \label{eq:s:L2}
    \begin{aligned}
        \int_{\RR^d} |s(t,x)|^2 \rho(t,x) dx & = \int_{\RR^d}|\EE\left((\gamma^{-1}(t) z|x_t = x\right)|^2 \rho(t,x)dx \\
        &\le \int_{\RR^d} \gamma^{-2}(t) \EE\left(|z|^2|x_t = x\right) \rho(t,x)dx\\
        &  \le d\gamma^{-2}(t).
    \end{aligned} 
\end{equation} 
This means that this integral is bounded for all $t\in(0,1)$. Since the integral is also continuous at $t=0$ and $t=1$, with values given by~\eqref{eq:rho0:1:sc}, it must be integrable on $[0,1]$ and~\eqref{eq:st:bounded} holds.

The objective $\mathcal{L}_s$ defined in~\eqref{eq:obj:s}  
can also be written as 
\begin{equation}
    \label{eq:EL:s}
    \begin{aligned}
     \mathcal{L}_s[\hat s] &= \int_0^1\int_{\RR^d} \left( \tfrac12|\hat s(t,x)|^2 + \gamma^{-1}(t)\EE\left(z|x_t=x\right)\cdot \hat s(t,x)\right) \rho(t,x) dxdt\\
    &= \int_0^1\int_{\RR^d} \left( \tfrac12 |\hat s(t,x)|^2 - s(t,x)\cdot \hat s(t,x) \right) \rho(t,x) dxdt,
    \end{aligned}
\end{equation}
where we used the definition of $s$ in~\eqref{eq:s:def}. This quadratic objective is bounded from below since
\begin{equation}
    \label{eq:EL:s:b}
    \begin{aligned}
    \mathcal{L}_s[\hat s] &= \tfrac12 \int_0^1\int_{\RR^d} \left|\hat s(t,x) - s(t,x) \right|^2 \rho(t,x) dxdt - \tfrac12  \int_0^1\int_{\RR^d} \left|s(t,x) \right|^2 \rho(t,x) dxdt\\
    & \ge - \tfrac12  \int_0^1\int_{\RR^d} \left|s(t,x) \right|^2 \rho(t,x) dxdt >-\infty
    \end{aligned}
\end{equation}
where the last inequality follows from~\eqref{eq:st:bounded}. Since $\rho$ is positive the  minimizer of \eqref{eq:EL:s} is unique and given by $\hat s = s$. 
\end{proof}


\interpolationfpe*
\begin{proof}
The forward FPE~~\eqref{eq:fpe} and the backward FPE~\eqref{eq:fpe:tr} are direct consequences of the TE~\eqref{eq:transport} and~\eqref{eq:s:def}, since the equality
\begin{equation}
    \label{eq:score}
\eps(t)\Delta \rho =  \eps(t)\nabla \cdot (\rho \nabla \log \rho ) = \eps(t)\nabla \cdot (s \rho ) 
\end{equation}
 can be used to convert between these equations.

\end{proof}

\subsection{Proof of Lemma~\ref{lem:reversed}}
\label{app:proof:generative}

\reversed*
\begin{proof}
    The SDE~\eqref{eq:sde:1} and the ODE~\eqref{eq:ode:1} are the evolution equations for the processes whose densities solve~\eqref{eq:fpe} and~\eqref{eq:transport}, respectively. The equation that requires some explanation is the backwards SDE~\eqref{eq:sde:R}, which can be solved backwards in time from $t=1$ to $t=0$. As discussed in the main text, by definition, its solution is $X^\rev_{t}=Z^\fwd_{1-t}$ where $Z^\fwd_t$ solves the forward SDE
\begin{equation}
    \label{eq:sde:generic:rev:Y}
    dZ^\fwd_t = -b_\rev(1-t,Z^\fwd_t)dt + \seps d W_t.
\end{equation}
To see how to write the backward It\^o formula~\eqref{eq:ito:formula}  note that given any  $f\in C^1([0,1];C_0^2(\RR^d))$, we have 
\begin{equation}
    \label{eq:Ito:rev:Y}
    \begin{aligned}
    df(1-t,Z^\fwd_t) &= -\partial_t f(1-t,Z^\fwd_t)dt+\nabla f(1-t,Z^\fwd_t) \cdot dZ^\fwd_t + \eps \Delta f(1-t,Z^\fwd_t) dt\\
    &= -\partial_t f(1-t,Z^\fwd_t)dt+\left( - b_\rev(1-t,Z^\fwd_t) \cdot \nabla f(1-t,Z^\fwd_t)  + \eps \Delta f(1-t,Z^\fwd_t)\right) dt\\
    & \quad+ \seps \nabla f(1-t,Z^\fwd_t) \cdot dW_t
     \end{aligned}
\end{equation}
In integral form, this equation can be written as
\begin{equation}
    \label{eq:int:Ito:rev:Y}
    \begin{aligned}
    f(1,Z^\fwd_{1}) &= f(1-t,Z^\fwd_{1-t}) \\
    &\quad- \int_{1-t}^1 \left( \partial_t f(1-s,Z^\fwd_s)+ b_\rev(1-s,Z^\fwd_s) \cdot \nabla f(1-s,Z^\fwd_s)  - \eps \Delta f(1-s,Z^\fwd_s)\right) ds \\
    & \quad - \seps \int_{1-t}^1 \nabla f(1-s,Z^\fwd_s) \cdot dW_s. 
     \end{aligned}
\end{equation}
Using $X^\rev_t = Z^\fwd_{1-t}$ and $W^\rev_t = - W_{1-t}$ and changing integration variable from $s$ to $1-s$, this is 
\begin{equation}
    \label{eq:int:Ito:rev:X}
    \begin{aligned}
    f(1,X^\rev_0) & = f(1-t,X^\rev_t) - \int_t^1 \left(\partial_t f(s,X^\rev_s) + b_\rev(s,X^\rev_s) \cdot \nabla f(s,X^\rev_s)  - \eps \Delta f(s,X^\rev_s)\right) ds \\
    & \quad - \seps \int_t^1 \nabla f(s,X^\rev_s) \cdot dW^\rev_s, 
     \end{aligned}
\end{equation}
In differential form, this is equivalent to saying that 
\begin{equation}
    \label{eq:Ito:rev}
    \begin{aligned}
    df(t,X^\rev_t) &= \partial_t f(t,X^\rev_t)dt + \nabla f(t,X^\rev_t) \cdot dX^\rev_t - \eps \Delta f(X^\rev_t) dt\\
    &= \left( \partial_t f(t,X^\rev_t) +b_\rev(t,X^\rev_t) \cdot \nabla f(t,X^\rev_t)  - \eps \Delta f(t,X^\rev_t)\right) dt + \seps \nabla f(t,X^\rev_t) \cdot dW_t
     \end{aligned}
\end{equation}
which is the backward It\^o formula~\eqref{eq:ito:formula}. Similarly, by the It\^o isometries we have: for any $g\in C^0([0,1];(C_0^0(\RR_d))^d)$ and $t\in[0,1]$
\begin{equation}
\label{eq:ito:iso:Y}
\EE^x \int_{1-t}^1 g(s,Z^\fwd_s) \cdot dW_s = 0, \qquad \EE^x \left|\int_{1-t}^1 g(s,Z^\fwd_s) \cdot dW_s\right|^2 =   \int_{1-t}^1 \EE^x|g (s,Z^\fwd_s) |^2ds.
\end{equation}
Written in terms of $X^\rev_t$, these are~\eqref{eq:ito:iso}.
\end{proof}


\paragraph{Derivation of \eqref{eq:ob:w:alt}.} For any $t\in[0,1]$ the score is the minimizer of
\begin{equation}
    \begin{aligned}
        &\int_{\RR^d} |\hat s(t,x) - \nabla \log \rho(t,x)|^2 \rho(t,x) dx\\
        &= \int_{\RR^d} \left(|\hat s(t,x)|^2 - 2\hat s(t,x) \cdot\nabla\log \rho(t,x)+ | \nabla \log \rho(t,x)|^2\right) \rho(t,x) dx \\
        &= \int_{\RR^d} \left(|\hat s(t,x)|^2 + 2\nabla \cdot \hat s(t,x) + | \nabla \log \rho(t,x)|^2\right) \rho(t,x) dx,
    \end{aligned}
\end{equation}
where we used the identity $\hat s \cdot \nabla \log \rho \,\rho = \hat s \cdot \nabla \rho$ and integration by parts to obtain the second equality. The last term involving $|\nabla \log \rho|^2$ is a constant in $\hat s$ that can be neglected for optimization. Expressing the remaining terms as an expectation over $x_t$ and integrating the result in time gives~\eqref{eq:ob:w:alt}.



\subsection{Proofs of Lemmas~\ref{lemma:kl_transport} and~\ref{lemma:kl_fpe}, and Theorem~\ref{thm:kl:bound}.}
\label{app:proof:kl}
\kltransport*
\begin{proof}
Using~\eqref{eq:2:te}, we compute analytically
\begin{align*}
    \frac{d}{dt}\KL{\rho(t)}{\hat\rho(t)} &= \frac{d}{dt}\int_{\RR^d} \log\left(\frac{\rho}{\hat \rho}\right)\rho dx\\
    %
    %
    &= \int_{\RR^d} \hat\rho\left(\frac{\partial_t \rho}{\hat\rho} - \frac{\rho}{(\hat\rho)^2}\partial_t\hat\rho\right)dx + \int\log\left(\frac{\rho}{\hat\rho}\right)\partial_t\rho dx\\
    %
    %
    &= -\int_{\RR^d} \left(\frac{\rho}{\hat\rho}\right)\partial_t\hat\rho dx + \int_{\RR^d}\log\left(\frac{\rho}{\hat\rho}\right)\partial_t\rho dx\\
    %
    %
    &= \int_{\RR^d} \left(\frac{\rho}{\hat\rho}\right)\nabla\cdot\left(\hat b\hat\rho\right)dx - \int\log\left(\frac{\rho}{\hat\rho}\right)\nabla\cdot\left(b\rho\right)dx\\
    %
    %
    &= -\int_{\RR^d} \nabla\left(\frac{\rho}{\hat\rho}\right)\cdot \hat b\hat\rho dx + \int\left(\nabla\log\rho - \nabla\log\hat\rho\right)\cdot b \rho dx\\
    %
    %
    &= -\int_{\RR^d} \left(\frac{\nabla\rho}{\hat\rho} - \frac{\rho\nabla\hat\rho}{\hat\rho^2}\right)\cdot \hat b\hat\rho dx 
    + \int_{\RR^d}\left(\nabla\log\rho - \nabla\log\hat\rho\right)\cdot b \rho dx\\
    %
    %
    &= \int_{\RR^d} \left(\nabla\log\hat\rho - \nabla\log\rho\right)\cdot \hat b \rho dx + \int_{\RR^d}\left(\nabla\log\rho - \nabla\log\hat\rho\right)\cdot b \rho dx\\
    %
    %
    &= \int_{\RR^d} \left(\nabla\log\hat\rho - \nabla\log\rho\right)\cdot (\hat b - b) \rho dx.
\end{align*}
where we omitted the argument $(t,x)$ of all functions for simplicity of notation. Integrating both sides from $0$ to $1$ completes the proof.
\end{proof}

\klfpe*
\begin{proof}
Similar to the proof of Lemma~\ref{lemma:kl_transport}, we can use the FPE in~\eqref{eq:2:fpe} to compute $\frac{d}{dt}\KL{\rho(t)}{\hat\rho(t)}$, which leads to the main result. Instead, we take a simpler approach, leveraging the result in Lemma~\ref{lemma:kl_transport}. We re-write the Fokker-Planck equations in~\eqref{eq:2:fpe} as the (score-dependent) transport equations
\begin{equation}
    \label{eqn:sde_transports}
    \begin{aligned}
        \partial_t \rho &= -\nabla\cdot((b_\fwd - \eps\nabla\log\rho)\rho),\\ 
        %
        \partial_t \hat\rho &= -\nabla\cdot((\hat b_\fwd - \eps\nabla\log\hat\rho)\hat\rho).
    \end{aligned}
\end{equation}
Applying Lemma~\ref{lemma:kl_transport} directly, we find that
\begin{equation}
\begin{aligned}
    \KL{\rho(1)}{\hat \rho(1)} = \int_0^1 \int_{\RR^d} \left[\left(\nabla\log\hat\rho  - \nabla\log\rho \right)\cdot \left(\left[\hat b_\fwd - \eps\nabla\log\hat\rho \right] - \left[b_\fwd - \eps\nabla\log\rho \right]\right)\right] \rho dx dt.
\end{aligned}
\end{equation}
Expanding the above,
\begin{equation}
\begin{aligned}
    \KL{\rho(1) }{\hat\rho(1) } &= \int_0^1 \int_{\RR^d}\left[\left(\nabla\log\hat\rho  - \nabla\log\rho \right)\cdot (\hat b_\fwd - b_\fwd)\right]\rho dx dt\\
    &\qquad  - \eps\int_0^1 \int_{\RR^d} \left[\norm{\nabla\log\rho  - \nabla\log\hat\rho }^2\right]\rho dx dt,
\end{aligned}
\end{equation}
which proves the first part of the lemma. Now, by Young's inequality, it holds for any fixed $\eta > 0$ that
\begin{align}
    \KL{\rho_1}{\hat\rho(1)} &\leq \frac{1}{2\eta}\int_0^1 \int_{\RR^d}\norm{\hat{b}_\fwd - b_\fwd}^2\rho_tdx dt\nonumber\\
    &\qquad  + \left(\tfrac{1}{2}\eta - \eps\right)\int_0^1 \int_{\RR^d} \norm{\nabla\log\rho_t - \nabla\log\hat{\rho}_t}^2\rho_tdxdt.
\end{align}
Hence, for $\eta =2\eps$, 
\begin{align}
    \label{eqn:kl_step}
    \KL{\rho_1}{\hat\rho(1)} &\leq \frac{1}{4\eps} \int_0^1 \int_{\RR^d}\norm{\hat{b}_\fwd - b_\fwd}^2\rho dxdt,
\end{align}
which proves the second part of the lemma.
\end{proof}

\likelihoodbound*
\begin{proof}
Observe that by Proposition~\ref{prop:generative}, the target density $\rho_1 =\rho(1, \cdot)$ is the density of the process $X_t$ that evolves according to SDE~\eqref{eq:sde:1}. By Lemma~\ref{lemma:kl_fpe} and an additional application of Young's inequality, we then have that 
\begin{align}
    \KL{\rho_1}{\hat\rho(1)} &\leq \frac{1}{2\eps}\int_0^1\int_{\RR^d}\left(|\hat{b} - b|^2 +\epsilon^2 \norm{s - \hat{s}}^2\right)\rho dxdt.
\end{align}
Using the definition of $\mathcal{L}_b[\hat{b}]$ and $\mathcal{L}_s[\hat{s}] $ in~\eqref{eq:obj:v}, and~\eqref{eq:obj:s}, we conclude that
\begin{equation}
    \KL{\rho_1}{\hat{\rho}(1)} \leq \frac{1}{2\eps}\left(\mathcal{L}_b[\hat{b}] - \min_{\hat{b}}\mathcal{L}_b[\hat{b}]\right)  + \frac{\eps}{2}\left(\mathcal{L}_s[\hat{s}] - \min_{\hat{s}}\mathcal{L}_s[\hat{s}]\right),
\end{equation}
which is~\eqref{eq:bound:kl}. Using that $b = v - \gamma\dot{\gamma} s$ and $\hat{b} = \hat{v} - \gamma\dot{\gamma}\hat{s}$, we can write~\eqref{eqn:kl_step} in this case as 
\begin{align}
    \KL{\rho_1}{\hat\rho(1)} &\leq \frac{1}{4\eps} \int_0^1 \int_{\RR^d}\norm{\hat{v} - v - (\gamma(t)\dot\gamma(t) - \epsilon)(\hat{s} - s)}^2\rho dxdt,\\
    %
    %
    &\leq \frac{1}{2\eps} \int_0^1 \int_{\RR^d}\left(\norm{\hat{v} - v}^2 + (\gamma(t)\dot\gamma(t) - \epsilon)^2\norm{\hat{s} - s}^2\right)\rho dxdt,\\
    %
    %
    &\leq \frac{1}{2\eps}\left(\mathcal{L}_{v}[\hat{v}] - \min_{\hat{v}}\mathcal{L}_{v}[\hat{v}]\right) + \frac{\max_{t\in [0 ,1]}(\gamma(t)\dot\gamma(t) - \epsilon)^2}{2\eps}\left(\mathcal{L}_{s}[\hat{s}] - \min_{\hat{v}}\mathcal{L}_{s}[\hat{s}]\right),
\end{align}
which is the final part of the theorem.
\end{proof}


\subsection{Proofs of Lemma~\ref{lem:tesol} and Theorem~\ref{prop:sde:rho}}
\label{app:prrof:de}

\TEs*
\begin{proof}
    If $\hat \rho$ solves the TE~\eqref{eq:TE:hat} and $ X_{s,t}$ solves the ODE~\eqref{eq:ode:st}, we have
\begin{equation}
    \label{eq:te:sol:1}
    \begin{aligned}
        \frac{d}{dt} \hat\rho(t,X_{s,t}(x)) & = \partial_t  \hat\rho(t, X_{s,t}(x))  + b_\ODE(t,X_{s,t}(x))  \cdot \nabla  \hat\rho(t,X_{s,t}(x))\\
        &= - \nabla \cdot b_\ODE(t, X_{s,t}(x)) \hat\rho(t,X_{s,t}(x))
    \end{aligned}
\end{equation}
This equation implies that
\begin{equation}
    \label{eq:te:sol:2}
    \begin{aligned}
        \frac{d}{dt} \left( \exp\left( \int_{s}^t\nabla \cdot b_\ODE(\tau,X_{s,\tau}(x)) d\tau \right) \hat\rho(t,X_{s,t}(x)) \right) = 0.
    \end{aligned}
\end{equation}
Integrating~\eqref{eq:te:sol:2} over $[0,t]$ and setting $s=t$ in the result gives
\begin{equation}
    \label{eq:te:sol:3:f}
    \begin{aligned}
          \hat\rho(t,x)  = \exp\left( -\int_0^{t}\nabla \cdot b_\ODE(\tau,X_{t,\tau}(x)) d\tau \right) \hat\rho(0,X_{t,0}(x))
    \end{aligned}
\end{equation}
If we use the initial condition $\hat \rho(0) = \rho_0$, this gives~\eqref{eq:TE:hat:s:f}.
Similarly, Integrating ~\eqref{eq:te:sol:2}  on $[t,1]$ and setting $s=t $ in the result gives
\begin{equation}
    \label{eq:te:sol:3:r}
    \begin{aligned}
         \hat\rho(t,x)  = \exp\left( \int_{t}^1\nabla \cdot b_\ODE(\tau,X_{t,\tau}(x)) d\tau \right) \hat\rho(1,X_{t,1}(x))
    \end{aligned}
\end{equation}
If we use the final condition $\hat \rho(1) = \rho_1$, this gives~\eqref{eq:TE:hat:s:b}.

\end{proof}
 

\FK*
\begin{proof}
Evaluating $d\hat \rho_\fwd(t, Y^\rev_t)$ via the backward It\^o formula~\eqref{eq:ito:formula} we obtain
\begin{equation}
    \label{eq:ito:rho:1}
    \begin{aligned}
    d\hat\rho_\fwd(t, Y_t^\rev) &= \partial_t \hat \rho_\fwd(t, Y_t^\rev) dt + \nabla \hat \rho_\fwd(t, Y_t^\rev) \cdot dY^\rev_t - \eps \Delta \hat \rho_\fwd(t,Y_t^\rev) dt\\
    &= \partial_t \hat \rho_\fwd(t, Y_t^\rev) dt + \nabla \hat \rho_\fwd(t, Y_t^\rev) \cdot \hat b_\fwd(t, Y^\rev_t)dt + \seps \nabla \hat \rho_\fwd(t, Y_t^\rev) \cdot dW^\rev_t - \eps \Delta \hat \rho_\fwd(t, Y_t^\rev) dt\\
    & = -\nabla \cdot \hat b_\fwd(t, Y^\rev_t)  \hat \rho_\fwd(t, Y^\rev_t) dt + \seps \nabla \hat\rho_\fwd(t, Y_t^\rev) \cdot dW^\rev_t.
    \end{aligned}
\end{equation}
where we used~\eqref{eq:sde:y:R} in the second step and~\eqref{eq:fpe:f:hat} in the last one. This equation can be written as a total differential in the form
\begin{equation}
    \label{eq:ito:rho:2}
    \begin{aligned}
    & d \left(\exp\left(- \int_t^1\nabla \cdot \hat b_\fwd(\tau, Y^\rev_\tau) d\tau\right)  \hat \rho_\fwd(t, Y_t^\rev) \right) \\
    & =  \seps \exp\left(-\int_t^1\nabla \cdot \hat b_\fwd(\tau, Y^\rev_\tau) d\tau\right) \nabla \hat \rho_\fwd(t, Y_t^\rev) \cdot dW^\rev_t,
    \end{aligned}
\end{equation}
which after integration over $t\in[0,1]$ becomes
\begin{equation}
    \label{eq:ito:rho:3}
    \begin{aligned}
    & \hat \rho_\fwd(1, Y_{t=1}^\rev) - \exp\left(- \int_0^1\nabla \cdot \hat b_\fwd(t, Y^\rev_t)  dt\right)  \rho_0(Y^\rev_{t=0}) \\
    %
    %
    & =  \seps \int_0^1 \exp\left(-\int_t^1\nabla \cdot \hat b_\fwd(\tau, Y^\rev_\tau) d\tau\right) \nabla \hat \rho(t, Y_t^\rev) \cdot dW^\rev_t.
    \end{aligned}
\end{equation}
where we used $\hat \rho(0)=\rho_0$.
Taking an expectation conditioned on the event $Y_{t=1}^\rev=x$ and using that the term on the right-hand side has mean zero, we find that
\begin{equation}
    \label{eq:ito:rho:4}
    \hat \rho_\fwd(1, Y_{t=1}^\rev) - \EE^x_\rev\exp\left(- \int_0^1\nabla \cdot \hat b_\fwd(t, Y^\rev_t)  dt\right)  \rho_0(Y^\rev_{t=0}) = 0.
\end{equation}
This gives~\eqref{eq:fk}.


Similarly, evaluating $d\hat \rho_\rev(t, Y^\fwd_t)$ via It\^o's formula, we obtain
\begin{equation}
    \label{eq:ito:rho:r:1}
    \begin{aligned}
    d \hat \rho_\rev(t, Y^\fwd_t) &= \partial_t \hat \rho_\rev(t, Y^\fwd_t) dt + \nabla \hat \rho_\rev(t, Y^\fwd_t) \cdot dY^\fwd_t + \epsilon \Delta \hat \rho_\rev(t, Y^\fwd_t) dt\\
    %
    %
    &= \partial_t \hat \rho_\rev(t, Y^\fwd_t) dt + \nabla \hat \rho_\rev(t, Y^\fwd_t) \cdot \hat b_\rev(t, Y^\fwd_t)dt + \seps \nabla \hat \rho_\rev(t, Y^\fwd_t) \cdot dW_t + \eps \Delta \hat \rho_\rev(t, Y^\fwd_t) dt\\
    %
    %
    & = -\nabla \cdot \hat b_\rev(t, Y^\fwd_t)   \hat \rho_\rev(t,Y^\fwd_t) dt + \seps \nabla \hat \rho_\rev(t,Y^\fwd_t) \cdot dW_t.
    \end{aligned}
\end{equation}
where we used~\eqref{eq:sde:y:1} in the second step and~\eqref{eq:fpe:r:hat} in the last one.  This equation can be written as a total differential in the form
\begin{equation}
    \label{eq:ito:rho:r:2}
    \begin{aligned}
    & d\left(\exp\left( \int_0^t\nabla \cdot \hat b_\rev(\tau, Y^\fwd_\tau)  d\tau\right)  \hat \rho_\rev(t,Y^\fwd_t) \right) \\
    %
    %
    & =  \seps \exp\left( \int_0^t\nabla \cdot \hat b_\rev(\tau, Y^\fwd_\tau)  d\tau\right) \nabla \hat \rho_\rev(t, Y^\fwd_t) \cdot dW_t.
    \end{aligned}
\end{equation}
Integrating the above on $t\in[0,1]$, we find that
\begin{equation}
    \label{eq:ito:rho:r:3}
    \begin{aligned}
    & \exp\left( \int_0^1\nabla \cdot \hat b_\rev(t, Y^\fwd_t)  dt\right)\rho_1( Y^\fwd_1) -   \hat \rho_\rev(0, Y^\fwd_{t=0})\\
    %
    %
    &= \seps \int_0^1 \exp\left( \int_0^t\nabla \cdot \hat b_\rev(\tau, Y^\fwd_\tau)  d\tau\right) \nabla \hat \rho_\rev(t,Y^\fwd_t) \cdot dW_t.
    \end{aligned}
\end{equation}
where we used $\hat \rho(1) = \rho_1$.
Taking an expectation conditioned on the event $Y^\fwd_{t=0} = x$ and applying the It\^o isometry, we deduce that
\begin{equation}
    \label{eq:ito:rho:r:4}
    \EE_\fwd^x \left(\exp\left( \int_0^1\nabla \cdot \hat b_\rev(t, Y^\fwd_t) dt\right)\rho_1(Y^\fwd_{t=1})\right) - \hat \rho_\rev(0, y) =0.
\end{equation}
This gives~\eqref{eq:fk:rev}.

\end{proof}


\subsection{Proof of Theorem~\ref{thm:diff}}
\label{app:diff}

\diffgen* 
\begin{proof}
Let us first consider what happens on the interval $t\in [0,\delta]$ where $I(t,x_0,x_1) = x_0$ by assumption and the stochastic interpolant~\eqref{eq:bb} with $x_0$ fixed  and $a(t)=a>0$ reduces to
\begin{equation}
\label{eq:bb:delta}
x_t = x_0 + \sqrt{2at(1-t)} z \quad \text{with} \quad x_0 \text{fixed}, \  z \sim {\sf N}(0,\Id).
\end{equation} 
The law of this process is simply
\begin{equation}
\label{eq:bb:delta:2}
x_t \sim {\sf N}(x_0,2at(1-t)\Id),
\end{equation} 
which means that its density satisfies for all $t>0$ the FPE
\begin{equation}
\label{eq:bb:delta:3}
\partial_t \rho + 2a t \nabla \cdot \left( s(t,x) \rho\right ) = a \Delta \rho,
\end{equation} 
where $s(t,x) = \nabla \log \rho(t,x)$ is the score, which is explicitly given by
\begin{equation}
\label{eq:score:delta}
s(t,x) = -\frac{x-x_0}{2at(1-t)}\qquad  \text{for} \ \ t\in (0,\delta].
\end{equation} 
This means that the drift term in the FPE~\eqref{eq:bb:delta:3} can be written as
\begin{equation}
\label{eq:score:delta:2}
2a t s(t,x) = -\frac{x-x_0}{1-t}\qquad \text{for} \ \ t\in (0,\delta].
\end{equation} 
and this equality also holds in the limit as $t\to0$. It is also easy to check that 
\begin{equation}
\label{eq:score:delta:3}
-\frac{x-x_0}{2at(1-t)} = -\frac{\sqrt{2at}}{\sqrt{(1-t)}} \EE(z|x_t=x)  \qquad \text{for} \ \ t\in [0,\delta]
\end{equation} 
which is consistent with the drift in~\eqref{eq:u:def:x0} on $t\in [0,\delta]$ since $\partial_t I(t,x_0,x_1) = 0$ on this interval. Since the right-hand side of \eqref{eq:score:delta:2} is nonsingular at $t=0$  it also means that the SDE~\eqref{eq:diff:sde} is well-defined for $t\in[0,\delta]$ and the law of its solutions coincide with that of the process $x_t$ defined~\eqref{eq:bb:delta}, $X_t^\diff \sim {\sf N}(x_0,2at(1-t)\Id)$ for $t\in[0,\delta]$. 

Considering next what happens on $t\in(\delta,1]$, since $\gamma(t) = \sqrt{2at(1-t)}$ is only zero at the endpoint $t=1$ where we assume that the density $\rho_1$ satisfies Assumption~\ref{as:rho:I} we can mimick all the arguments in the proof of of Theorem~\ref{prop:interpolate} and Corollaries~\ref{prop:interpolate_fpe} and \ref{prop:generative} to terminate the proof.
\end{proof}

Note that this proof shows that is enough to have $\partial_t I(t=0,x_0,x_1) = 0$, since this implies that $I(t,x_0,x_1) = x_0 + O(t^2)$. It also shows that, while it is key to use an SDE on $t\in[0,\delta]$ so that the generative process can spread the mass away from $x_0$, the diffusive noise is no longer necessary and we could switch back to a probability flow ODE on $t\in (\delta,1]$ (using a time-dependent $a(t)$ to that effect with $a(t)=0$ for $t\in (\delta,1]$).

\subsection{Proof of Lemma~\ref{lem:interp} and Theorem~\ref{prop:sb}.}
\label{app:sb}

\interppdf*
\begin{proof}
By definition of the map~$T$ in~\eqref{eq:interpol}, if $x_0\sim \rho_0$ and $x_1\sim\rho_1$, then $T^{-1}(0,x_0)\sim {\sf N}(0,\Id)$ and $T^{-1}(1,x_1)\sim {\sf N}(0,\Id)$. As a result, since $x_0$, $x_1$, and $z$ are independent, and $z\sim {\sf N}(0,\Id)$,  we have
\begin{equation}
    \label{eq:gauss:sum}
    \alpha(t) T^{-1}(0,x_0) + \beta(t) T^{-1}(1,x_1) +\gamma(t) z \sim {\sf N}(0,\alpha^2(t)\Id+\beta^2(t)\Id+\gamma^2(t)\Id) = {\sf N}(0,\Id)
\end{equation}
where the second equality follows from the condition $\alpha^2(t)+\beta^2(t)+\gamma^2(t)=1$. Therfore, using again the definition of the map~$T$
\begin{equation}
    \label{eq:rho:map}
    T(t,\alpha(t) T^{-1}(0,x_0) + \beta(t) T^{-1}(1,x_1) +\gamma(t) z) \sim \rho(t),
\end{equation}
and we are done.
\end{proof}

\schrob*
\begin{proof}
Denote by $\hat\rho(t,x)$ the density of $\hat x_t$ and define the current $\hat\jmath : [0,1]\times \RR^d \to \RR^d$ as
\begin{equation}
    \label{eq:def:rho:j}
        \hat \jmath(t,x) = \EE \big(\partial_t \hat I_t +\gamma(t) z| x=\hat x_t \big) \hat \rho(t,x).
\end{equation}
The max-min problem~\eqref{eq:max:min} can then be formulated as the constrained optimization problem:
\begin{equation}
    \label{eq:max:min:rho:j}
    \begin{aligned}
        \max_{\hat \rho,\hat \jmath} \min_{\hat u} &\int_0^1 \int_{\RR^d}\left( \tfrac12|\hat u(t,x)|^2 \hat \rho(t,x)-  \hat u(t,x)\cdot \hat \jmath(t,x) \right) dx dt\\
        \text{subject to:} \quad & \partial_t \hat \rho + \nabla \cdot \hat \jmath= \eps  \Delta \hat\rho, \quad \hat\rho(t=0) = \rho_0, \quad \hat \rho(t=1)= \rho_1
    \end{aligned}
\end{equation}
To solve this problem we can use the extended objective
\begin{equation}
    \label{eq:max:min:rho:j:2}
    \begin{aligned}
        \max_{\hat \rho,\hat \jmath} \min_{\hat u} &\Bigg(\int_0^1 \int_{\RR^d}\left( \tfrac12|\hat u(t,x)|^2 \hat \rho(t,x)-  \hat u(t,x)\cdot \hat \jmath(t,x) \right) dx dt\\
        & -\int_0^1 \int_{\RR^d}\lambda(t,x) \left(\partial_t \hat \rho(t,x) + \nabla \cdot \hat \jmath(t,x) - \eps  \Delta \hat\rho(t,x)\right) dx dt \\
        & +\int_{\RR^d} \eta_0(x)\left(\hat\rho(0,x)-\rho_0(x)\right) dx-\int_{\RR^d} \eta_1(x)\left(\hat\rho(1,x)-\rho_1(x)\right) dx \Bigg)
    \end{aligned}
\end{equation}
where $\lambda(t,x)$, $\eta_0(x)$, and $\eta_1(x)$ are Lagrange multipliers used to enforce the constraints. The unique minimizer $(\rho,j,\lambda)$ of this optimization problem solves the Euler-Lagrange equations:
\begin{equation}
    \label{eq:max:min:rho:j:el}
    \begin{aligned}
        & \partial_t \rho + \nabla \cdot j = \eps  \Delta \rho, \quad \rho(t=0) = \rho_0\quad \rho(t=1) = \rho_1\\
        & \partial_t \lambda + \tfrac12 |u|^2 = -\eps \Delta \lambda,\\
        & j = u \rho \\
        & u = \nabla \lambda
    \end{aligned}
\end{equation}
We can use the last two equations to write the first two as \eqref{eq:max:min:rho:j:el:2}, with $u=\nabla \lambda$. Since under Assumption~\ref{as:sb:2} there is an interpolant that realizes the density $\rho(t)$ that solves~\eqref{eq:max:min:rho:j:el:2}, we conclude that an optimizer $(I,u)$ of the the max-min problem~\eqref{eq:max:min} exists. For any optimizer, $I$ will be such that $\rho(t)$ is the density of $x_t= I(t,x_0,x_1)+\gamma(t) z$, and $u$ will satisfy $u=\nabla \lambda$.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:denoise:iter}}
\label{app:denoise}

\dn*

\begin{proof}
    Use 
    \begin{equation}
    \label{eq:betaa:expand}
    \beta(t_{j+1}) \beta^{-1}(t_j) = 1 + \dot\beta(t_j) \beta^{-1}(t_i) (t_{j+1}-t_j) + O\big((t_{j+1}-t_j)^2\big)
\end{equation}
and
\begin{equation}
    \label{eq:alpha:expand}
    \alpha(t_{j+1}) - \alpha(t_j)\beta(t_{j+1}) \beta^{-1}(t_j) = \big( \dot\alpha(t_j) - \alpha(t_j) \dot\beta(t_j) \beta^{-1}(t_i)) (t_{j+1}-t_j) + O\big((t_{j+1}-t_j)^2\big)
\end{equation}
to deduce that \eqref{eq:iterate} implies
\begin{equation}
    \label{eq:iterate:expand}
    \begin{aligned}
    X^\DEN_{{j+1}} & = X^\DEN_{j} + \dot \beta(t_{j}) \beta^{-1}(t_j) X^\DEN_{j} (t_{j+1}-t_j)\\
    & + \big( \dot\alpha(t_j) - \alpha(t_j) \dot\beta(t_j) \beta^{-1}(t_i))   \eta^\OS_z(t_{j},X^\DEN_{j})(t_{j+1}-t_j)  + O\big((t_{j+1}-t_j)^2\big).
    \end{aligned}
\end{equation}
or equivalently
\begin{equation}
    \label{eq:iterate:expand:2}
    \begin{aligned}
    \frac{X^\DEN_{{j+1}} -X^\DEN_{j}}{t_{j+1}-t_j} = \dot \beta(t_{j}) \beta^{-1}(t_j) X^\DEN_{j} + \big( \dot\alpha(t_j) - \alpha(t_j) \dot\beta(t_j) \beta^{-1}(t_i))   \eta^\OS_z(t_{j},X^\DEN_{j})  + O(t_{j+1}-t_j).
    \end{aligned}
\end{equation}
Taking the limit as $N,j\to \infty$ with $j/N\to t\in[0,1]$, we recover~\eqref{eq:iterate:lim} and deduce that $X^\DEN_{j} \to X_t$.
\end{proof}

Note that the proof shows that the result of Theorem~\ref{thm:denoise:iter} also holds if we use a nonuniform grid of times $t_j$, $j\in\{1,\ldots,N\}$.


\subsection{Proof of Theorem~\ref{thm:cond}}
\label{app:rect}

\rec*

\begin{proof}
The first part of the statement can be established by following the same steps as in the proof of Theorem~\ref{prop:interpolate} and Corollary~\ref{prop:interpolate_fpe}. For the proof of the second part, use first \eqref{eq:new:os} written as $x^\REC_t = M(t,z)$ in \eqref{eq:b:explicit} to deduce that
\begin{equation}
    \label{eq:b:explicit:int}
    b^\REC(t,x^\REC_t) = \dot \alpha(t) N(t,M(t,z)) + \dot\beta(t) X_{t=1}(N(t,M(t,z))) = \dot \alpha(t) z + \dot\beta(t) X_{t=1}(z) = \dot x^\REC_t
\end{equation}
This shows that~\eqref{eq:b:explicit} is the unique minimizer of~\eqref{eq:obj:brec}. Next, use \eqref{eq:prod:flow:sol} written as $X^\REC_t(x) = M(t,x)$ in \eqref{eq:b:explicit} to deduce that
\begin{equation}
    \label{eq:b:explicit:2}
    b^\REC(t,X^\REC_t(x)) = \dot \alpha(t) N(t,M(t,x)) + \dot\beta(t) X_{t=1}(N(t,M(t,x))) = \dot \alpha(t) x + \dot\beta(t) X_{t=1}(x)
\end{equation}
This implies that~\eqref{eq:prod:flow:sol} solves \eqref{eq:prob:flow:rec}, and since the solution of this ODE is unique, we are done.
\end{proof}

\begin{table}[!t]
\centering
\begin{tabular}{lccc}
\toprule &  ImageNet 32$\times$32 & Flowers & Flowers (Mirror)   \\
\midrule Dimension & 32$\times$32 & 128$\times$128 &  128$\times$128\\

\# Training point &  1,281,167  & 315,123 & 315,123 \\
\midrule Batch Size & 512 & 64 & 64 \\
Training Steps  & 8$\times$10$^5$  & 3.5 $\times$ 10$^5$  & 8 $\times$ 10$^5$  \\
Hidden dim  & 256 & 128 & 128 \\
Attention Resolution  & 64 & 64 & 64 \\
Learning Rate (LR) & $0.0002$ & $0.0002$ & $0.0002$\\
LR decay (1k epochs) & 0.995 & 0.995 & 0.985  \\
U-Net dim mult & [1,2,2,2] & [1,1,2,3,4]  & [1,1,2,3,4]  \\
Learned $t$ sinusoidal embedding  & Yes & Yes & Yes \\
$t_{0}, t_{f}$ for $t\sim \text{Unif}[t_0, t_f]$, learning $\eta$ & [0.0, 1.0] & n/a &  n/a \\
$t_{0}, t_{f}$ for $t\sim \text{Unif}[t_0, t_f]$, learning $s$ & n/a & [.0002, .9998] & [.0002, .9998] \\
$t_{0}, t_{f}$ when sampling with ODE & [0.0, 1.0] & [0.0001, 0.9999] & [.0001, .9999] \\
$t_{0}, t_{f}$ when sampling with SDE, $\eta$ & [0.0, 0.97] + denoising & n/a & n/a \\
$t_{0}, t_{f}$ when sampling with SDE, $s$ & n/a & [.0001, .9999] & [.0001, .9999] \\
$\gamma(t)$ in $x_t$ & $\sqrt{(t(1-t)}$ & $\sqrt{(t(1-t)}$ & $\sqrt{(10t(1-t)}$ \\
EMA decay rate & 0.9999 & 0.9999 & 0.9999 \\
EMA start iteration & 10000 & 10000 & 10000 \\
$\#$ GPUs & 2 & 4 & 2\\
\bottomrule
\end{tabular}
\caption{Hyperparameters and architecture for image datasets.}
\label{tab:archs:img}
\end{table}

\section{Experimental Specifications}
\label{app:exp}

Details for the experiments in Section~\ref{sec:sde:ode} are provided here. Feed forward neural networks of depth $4$ and width $512$ are used for each model of the velocities $b$, $v$, and $s$. Training was done for $7000$ iterations on batches comprised of $25$ draws from the base, $400$ draws from the target, and $100$ time slices. At each iteration, we used a variance reduction technique based on antithetic sampling, in which two samples $\pm z$ are used for each evaluation of the loss. The objectives given in \eqref{eq:obj:v} and \eqref{eq:obj:s} were optimized using the Adam optimizer. The learning rate was set to $.002$ and was dropped by a factor of $2$ every $1500$ iterations of training. To integrate the ODE/SDE when drawing samples, we used the Heun-based integrator as suggested in \cite{Karras2022edm}.

\subsection{Image Experiments}
\label{app:exp:img}

\paragraph{Network Architectures.} For all image generation experiments, the U-Net architecture originally proposed in \cite{ho2020} is used. The specification of architecture hyperparameters as well as training hyperparameters are given in Table \ref{tab:archs:img}. The same architecture is used regardless of whether learning $b, v, s,$ or $\eta$. 

When using the SDE and learning $\eta_z$, we found that integrating to a time slightly before $t_f=1.0$ and using the denoising formula~\eqref{eq:iterate} beyond this point provided the best results, as described in the main text.