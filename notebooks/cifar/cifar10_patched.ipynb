{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a5fe6b",
   "metadata": {},
   "source": [
    "cifar10 patch reconstruction using stochastic interpolants\n",
    "\n",
    "this implementation:\n",
    "- trains on dog class only (cifar10 class 5)\n",
    "- uses mask-conditioned u-net architecture\n",
    "- learns to reconstruct masked patches while keeping visible pixels fixed\n",
    "- conditions on mask during both training and generation\n",
    "- forces visible pixels to remain constant in output\n",
    "\n",
    "approach:\n",
    "  x0 = masked_image * mask + noise * (1-mask)  # we start with visible pixels + noise in masked regions\n",
    "  x1 = original_image                           # we target full image\n",
    "  model learns interpolant: x0 -> x1 conditioned on mask\n",
    "  during generation: output = model_output * (1-mask) + original * mask  # we force visible pixels fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acaa3739",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available, setting default tensor residence to gpu\n",
      "cuda\n",
      "torch version: 2.9.0+cu128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janis/4A/probgraph/probgraph/.venv/lib/python3.12/site-packages/torch/__init__.py:1275: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from typing import Tuple, Any\n",
    "\n",
    "import interflow as itf\n",
    "import interflow.stochastic_interpolant as stochastic_interpolant\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available, setting default tensor residence to gpu')\n",
    "    itf.util.set_torch_device('cuda')\n",
    "else:\n",
    "    print('no cuda device found')\n",
    "print(itf.util.get_torch_device())\n",
    "\n",
    "print(\"torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f87e01",
   "metadata": {},
   "source": [
    "we define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6c0399",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def grab(var):\n",
    "    \"\"\"we take a tensor off the gpu and convert it to a numpy array on the cpu\"\"\"\n",
    "    return var.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f536cec",
   "metadata": {},
   "source": [
    "we load cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa637ef3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:06<00:00, 27.4MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "filtered dataset to class 'dog': 5000 images (out of 50000 total)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"../../data/cifar10\", train=True, \n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# we filter dataset to keep only dog class (class 5)\n",
    "# cifar10 classes: 0=airplane, 1=automobile, 2=bird, 3=cat, 4=deer, 5=dog, 6=frog, 7=horse, 8=ship, 9=truck\n",
    "target_class = 5  # we select dog class\n",
    "dog_indices = [i for i in range(len(trainset)) if trainset.targets[i] == target_class]\n",
    "print(f\"\\nfiltered dataset to class 'dog': {len(dog_indices)} images (out of {len(trainset)} total)\")\n",
    "\n",
    "# we create data iterator that only samples from dog images\n",
    "def get_cifar_batch(bs):\n",
    "    \"\"\"we get a batch of cifar10 dog images only\"\"\"\n",
    "    indices = torch.randint(0, len(dog_indices), (bs,))\n",
    "    imgs = torch.stack([trainset[dog_indices[i]][0] for i in indices])\n",
    "    return imgs.to(itf.util.get_torch_device())\n",
    "\n",
    "# we create masking function for patches\n",
    "def create_patch_mask(bs, patch_size=8, num_patches=4):\n",
    "    \"\"\"we create random patch masks, 1 for visible pixels, 0 for masked patches\"\"\"\n",
    "    mask = torch.ones(bs, 3, 32, 32)\n",
    "    for i in range(bs):\n",
    "        for _ in range(num_patches):\n",
    "            x = torch.randint(0, 32 - patch_size, (1,)).item()\n",
    "            y = torch.randint(0, 32 - patch_size, (1,)).item()\n",
    "            mask[i, :, x:x+patch_size, y:y+patch_size] = 0  # we mask the patch\n",
    "    return mask.to(itf.util.get_torch_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64d488",
   "metadata": {},
   "source": [
    "we define u-net style convolutional denoiser for image reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c202569",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class UNetDenoiser(nn.Module):\n",
    "    \"\"\"we use u-net architecture with skip connections for image reconstruction, conditioned on mask\"\"\"\n",
    "    def __init__(self, in_channels=5, out_channels=3, base_channels=64):  # we add 1 channel for mask conditioning\n",
    "        super().__init__()\n",
    "        \n",
    "        # we define encoder (downsampling path)\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels*2, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*2, base_channels*2, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*2, base_channels*4, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*4, base_channels*4, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # we define bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*4, base_channels*8, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*8, base_channels*8, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # we define decoder (upsampling path) with skip connections\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*8, base_channels*4, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec3_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*8, base_channels*4, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*4, base_channels*4, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*4, base_channels*2, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec2_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*4, base_channels*2, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*2, base_channels*2, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*2, base_channels, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec1_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*2, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # we define final output layer\n",
    "        self.final = nn.Conv2d(base_channels, out_channels, 1)\n",
    "    \n",
    "    def forward(self, x_with_t):\n",
    "        \"\"\"we forward pass with skip connections\"\"\"\n",
    "        # we encode\n",
    "        e1 = self.enc1(x_with_t)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        \n",
    "        # we process bottleneck\n",
    "        b = self.bottleneck(e3)\n",
    "        \n",
    "        # we decode with skip connections\n",
    "        d3 = self.dec3(b)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3_conv(d3)\n",
    "        \n",
    "        d2 = self.dec2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2_conv(d2)\n",
    "        \n",
    "        d1 = self.dec1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1_conv(d1)\n",
    "        \n",
    "        # we output final reconstruction\n",
    "        out = self.final(d1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7c2b5",
   "metadata": {},
   "source": [
    "we define wrapper for eta network to match expected interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e1bd3f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EtaNetwork(nn.Module):\n",
    "    \"\"\"we wrap unet to accept concatenated [x, t, mask] input\"\"\"\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.mask = None  # we store mask for conditioning\n",
    "    \n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"we set the mask for conditioning\"\"\"\n",
    "        self.mask = mask\n",
    "    \n",
    "    def forward(self, xt_concat):\n",
    "        \"\"\"we expect input of shape [bs, 3*32*32 + 1]\"\"\"\n",
    "        bs = xt_concat.shape[0]\n",
    "        x = xt_concat[:, :-1].reshape(bs, 3, 32, 32)  # we reshape to image\n",
    "        t = xt_concat[:, -1:]  # we extract time\n",
    "        \n",
    "        # we expand time to match spatial dimensions\n",
    "        t_channel = t.view(bs, 1, 1, 1).expand(bs, 1, 32, 32)\n",
    "        \n",
    "        # we add mask conditioning (use first channel of mask for simplicity)\n",
    "        if self.mask is not None:\n",
    "            mask_channel = self.mask[:bs, 0:1, :, :]  # we take first channel of mask [bs, 1, 32, 32]\n",
    "        else:\n",
    "            mask_channel = torch.ones(bs, 1, 32, 32).to(x.device)  # we default to all visible\n",
    "        \n",
    "        x_with_t_mask = torch.cat([x, t_channel, mask_channel], dim=1)  # we concatenate [x, t, mask]\n",
    "        \n",
    "        # we process through unet\n",
    "        out = self.unet(x_with_t_mask)\n",
    "        \n",
    "        # we flatten output\n",
    "        return out.reshape(bs, -1)\n",
    "\n",
    "# we define velocity field wrapper\n",
    "class VelocityNetwork(nn.Module):\n",
    "    \"\"\"we wrap unet for velocity field b, conditioned on mask\"\"\"\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.mask = None  # we store mask for conditioning\n",
    "    \n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"we set the mask for conditioning\"\"\"\n",
    "        self.mask = mask\n",
    "    \n",
    "    def forward(self, xt_concat):\n",
    "        \"\"\"we expect input of shape [bs, 3*32*32 + 1]\"\"\"\n",
    "        bs = xt_concat.shape[0]\n",
    "        x = xt_concat[:, :-1].reshape(bs, 3, 32, 32)  # we reshape to image\n",
    "        t = xt_concat[:, -1:]  # we extract time\n",
    "        \n",
    "        # we expand time to match spatial dimensions\n",
    "        t_channel = t.view(bs, 1, 1, 1).expand(bs, 1, 32, 32)\n",
    "        \n",
    "        # we add mask conditioning (use first channel of mask for simplicity)\n",
    "        if self.mask is not None:\n",
    "            mask_channel = self.mask[:bs, 0:1, :, :]  # we take first channel of mask [bs, 1, 32, 32]\n",
    "        else:\n",
    "            mask_channel = torch.ones(bs, 1, 32, 32).to(x.device)  # we default to all visible\n",
    "        \n",
    "        x_with_t_mask = torch.cat([x, t_channel, mask_channel], dim=1)  # we concatenate [x, t, mask]\n",
    "        \n",
    "        # we process through unet\n",
    "        out = self.unet(x_with_t_mask)\n",
    "        \n",
    "        # we flatten output\n",
    "        return out.reshape(bs, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d7129",
   "metadata": {},
   "source": [
    "we define training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c459561",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    bs: int,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    opt_b: Any,\n",
    "    opt_eta: Any,\n",
    "    sched_b: Any,\n",
    "    sched_eta: Any,\n",
    "    patch_size: int,\n",
    "    num_patches: int,\n",
    "    mask_loss_weight: float = 10.0\n",
    "):\n",
    "    \"\"\"we take a single step of optimization on the training set\"\"\"\n",
    "    opt_b.zero_grad()\n",
    "    opt_eta.zero_grad()\n",
    "    \n",
    "    # we construct batch of real images\n",
    "    x1s_img = get_cifar_batch(bs)  # we get [bs, 3, 32, 32]\n",
    "    \n",
    "    # we create masks\n",
    "    masks = create_patch_mask(bs, patch_size=patch_size, num_patches=num_patches)  # we get [bs, 3, 32, 32]\n",
    "    \n",
    "    # we create masked images + noise in masked regions as starting point\n",
    "    noise = torch.randn_like(x1s_img) * (1 - masks)  # we add noise only in masked regions\n",
    "    x0s_img = x1s_img * masks + noise  # we combine masked image and noise\n",
    "    \n",
    "    # we flatten for interpolant\n",
    "    x0s = x0s_img.reshape(bs, -1)  # we flatten to [bs, 3072]\n",
    "    x1s = x1s_img.reshape(bs, -1)  # we flatten to [bs, 3072]\n",
    "    masks_flat = masks.reshape(bs, -1)  # we flatten mask too\n",
    "    \n",
    "    # we sample random times\n",
    "    ts = torch.rand(size=(bs,)).to(itf.util.get_torch_device())\n",
    "    \n",
    "    # we set masks for conditioning in both networks\n",
    "    b.set_mask(masks)\n",
    "    eta.set_mask(masks)\n",
    "    \n",
    "    # we compute the losses\n",
    "    loss_start = time.perf_counter()\n",
    "    loss_b_full = loss_fn_b(b, x0s, x1s, ts, interpolant)\n",
    "    loss_eta_full = loss_fn_eta(eta, x0s, x1s, ts, interpolant)\n",
    "    \n",
    "    # we weight the loss to focus on masked regions (multiply by mask_loss_weight for masked pixels)\n",
    "    # we compute per-pixel weight: visible pixels get weight 1.0, masked pixels get weight mask_loss_weight\n",
    "    loss_weights = masks_flat + mask_loss_weight * (1 - masks_flat)  # we create per-pixel weights\n",
    "    \n",
    "    # we apply weighted loss (approximation: we multiply total loss by average weight)\n",
    "    avg_weight = loss_weights.mean()\n",
    "    loss_b = loss_b_full * avg_weight\n",
    "    loss_eta = loss_eta_full * avg_weight\n",
    "    \n",
    "    loss_val = loss_b + loss_eta\n",
    "    loss_end = time.perf_counter()\n",
    "    \n",
    "    # we compute the gradient\n",
    "    backprop_start = time.perf_counter()\n",
    "    loss_b.backward()\n",
    "    loss_eta.backward()\n",
    "    b_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(b.parameters(), float('inf'))])\n",
    "    eta_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(eta.parameters(), float('inf'))])\n",
    "    backprop_end = time.perf_counter()\n",
    "    \n",
    "    # we perform the update\n",
    "    update_start = time.perf_counter()\n",
    "    opt_b.step()\n",
    "    opt_eta.step()\n",
    "    sched_b.step()\n",
    "    sched_eta.step()\n",
    "    update_end = time.perf_counter()\n",
    "    \n",
    "    if counter < 5:\n",
    "        print(f'[loss: {loss_end - loss_start:.4f}s], [backprop: {backprop_end-backprop_start:.4f}s], [update: {update_end-update_start:.4f}s]')\n",
    "    \n",
    "    return loss_val.detach(), loss_b.detach(), loss_eta.detach(), b_grad.detach(), eta_grad.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d360da",
   "metadata": {},
   "source": [
    "we define visualization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca89a47",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_plots(\n",
    "    b: torch.nn.Module,\n",
    "    eta: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    counter: int,\n",
    "    data_dict: dict,\n",
    "    patch_size: int,\n",
    "    num_patches: int\n",
    "):\n",
    "    \"\"\"we make plots to visualize reconstruction results\"\"\"\n",
    "    print(f\"\\nepoch: {counter}\")\n",
    "    \n",
    "    # we get a batch for visualization\n",
    "    vis_bs = 8\n",
    "    x1s_img = get_cifar_batch(vis_bs)\n",
    "    masks = create_patch_mask(vis_bs, patch_size=patch_size, num_patches=num_patches)\n",
    "    \n",
    "    # we create masked images\n",
    "    noise = torch.randn_like(x1s_img) * (1 - masks)\n",
    "    x0s_img = x1s_img * masks + noise\n",
    "    \n",
    "    # we reconstruct using probability flow\n",
    "    x0s = x0s_img.reshape(vis_bs, -1)\n",
    "    x1s = x1s_img.reshape(vis_bs, -1)\n",
    "    \n",
    "    # we use simple forward integration\n",
    "    with torch.no_grad():\n",
    "        # we set masks for conditioning during generation\n",
    "        b.set_mask(masks)\n",
    "        eta.set_mask(masks)\n",
    "        \n",
    "        s = stochastic_interpolant.SFromEta(eta, interpolant.a)\n",
    "        pflow = stochastic_interpolant.PFlowIntegrator(\n",
    "            b=b, method='dopri5', interpolant=interpolant, n_step=10\n",
    "        )\n",
    "        xfs_pflow, _ = pflow.rollout(x0s)\n",
    "        xf_pflow_raw = xfs_pflow[-1].reshape(vis_bs, 3, 32, 32)\n",
    "        \n",
    "        # we force visible pixels to remain fixed (only reconstruct masked regions)\n",
    "        xf_pflow = xf_pflow_raw * (1 - masks) + x1s_img * masks  # we keep original pixels where mask=1\n",
    "    \n",
    "    # we plot results\n",
    "    fig, axes = plt.subplots(3, vis_bs, figsize=(vis_bs*2, 6))\n",
    "    \n",
    "    for i in range(vis_bs):\n",
    "        # we denormalize images for visualization\n",
    "        def denorm(img):\n",
    "            return img * 0.5 + 0.5\n",
    "        \n",
    "        # we show original image\n",
    "        axes[0, i].imshow(np.transpose(grab(denorm(x1s_img[i])), (1, 2, 0)))\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('original', fontsize=10)\n",
    "        \n",
    "        # we show masked image\n",
    "        axes[1, i].imshow(np.transpose(grab(denorm(x0s_img[i])), (1, 2, 0)))\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('masked', fontsize=10)\n",
    "        \n",
    "        # we show reconstructed image\n",
    "        axes[2, i].imshow(np.transpose(grab(denorm(xf_pflow[i])), (1, 2, 0)))\n",
    "        axes[2, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[2, i].set_title('reconstructed', fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f'dog patch reconstruction - epoch {counter}', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'dog_reconstruction_epoch_{counter}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # we plot training curves\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    epochs = np.arange(len(data_dict['losses'])) * metrics_freq\n",
    "    \n",
    "    # we plot losses\n",
    "    axes[0].plot(epochs, data_dict['losses'], label='total loss', linewidth=2)\n",
    "    axes[0].plot(epochs, data_dict['b_losses'], label='b loss', alpha=0.7)\n",
    "    axes[0].plot(epochs, data_dict['eta_losses'], label='eta loss', alpha=0.7)\n",
    "    axes[0].set_xlabel('epoch')\n",
    "    axes[0].set_ylabel('loss')\n",
    "    axes[0].set_title('training loss (dog class)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # we plot gradients\n",
    "    axes[1].plot(epochs, data_dict['b_grads'], label='b grad norm', linewidth=2)\n",
    "    axes[1].plot(epochs, data_dict['eta_grads'], label='eta grad norm', linewidth=2)\n",
    "    axes[1].set_xlabel('epoch')\n",
    "    axes[1].set_ylabel('gradient norm')\n",
    "    axes[1].set_title('gradient norms')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # we plot learning rates\n",
    "    axes[2].plot(epochs, data_dict['lrs'], label='learning rate', linewidth=2)\n",
    "    axes[2].set_xlabel('epoch')\n",
    "    axes[2].set_ylabel('learning rate')\n",
    "    axes[2].set_title('learning rate schedule')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'dog_training_curves_epoch_{counter}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e73d1",
   "metadata": {},
   "source": [
    "we define main training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90c42fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hyperparameters:\n",
      "  class: dog (cifar10 class 5)\n",
      "  batch_size: 32\n",
      "  learning_rate: 0.0001\n",
      "  n_epochs: 5000\n",
      "  patch_size: 8\n",
      "  num_patches: 4\n",
      "\n",
      "conditioning approach:\n",
      "  - networks conditioned on binary mask (visible=1, masked=0)\n",
      "  - visible pixels kept fixed during generation\n",
      "  - only masked regions are reconstructed\n",
      "\n",
      "using interpolant: one-sided-linear\n",
      "\n",
      "creating u-net architectures with mask conditioning...\n",
      "b network parameters: 8,565,315\n",
      "eta network parameters: 8,565,315\n",
      "\n",
      "starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janis/4A/probgraph/probgraph/refs/former/former_code/stochastic_interpolants/interflow/stochastic_interpolant.py:83: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  Class for all things interpoalnt $x_t = I_t(x_0, x_1) + \\gamma(t)z.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "VelocityNetwork.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     68\u001b[39m counter = \u001b[32m1\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     loss, b_loss, eta_loss, b_grad, eta_grad = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_eta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msched_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msched_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patches\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# we log metrics\u001b[39;00m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (counter - \u001b[32m1\u001b[39m) % metrics_freq == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(bs, interpolant, opt_b, opt_eta, sched_b, sched_eta, patch_size, num_patches, mask_loss_weight)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# we compute the losses\u001b[39;00m\n\u001b[32m     39\u001b[39m loss_start = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m loss_b_full = \u001b[43mloss_fn_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m loss_eta_full = loss_fn_eta(eta, x0s, x1s, ts, interpolant)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# we weight the loss to focus on masked regions (multiply by mask_loss_weight for masked pixels)\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# we compute per-pixel weight: visible pixels get weight 1.0, masked pixels get weight mask_loss_weight\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/4A/probgraph/probgraph/refs/former/former_code/stochastic_interpolants/interflow/stochastic_interpolant.py:754\u001b[39m, in \u001b[36mmake_loss.<locals>.loss\u001b[39m\u001b[34m(bvseta, x0s, x1s, ts, interpolant)\u001b[39m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(\n\u001b[32m    746\u001b[39m     bvseta: Velocity, \n\u001b[32m    747\u001b[39m     x0s: torch.tensor, \n\u001b[32m   (...)\u001b[39m\u001b[32m    750\u001b[39m     interpolant: Interpolant,\n\u001b[32m    751\u001b[39m ) -> torch. tensor:\n\u001b[32m    753\u001b[39m     loss_fn = make_batch_loss(loss_fn_unbatched, method)\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m     loss_val = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbvseta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    755\u001b[39m     loss_val = loss_val.mean()\n\u001b[32m    756\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_val\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/4A/probgraph/probgraph/.venv/lib/python3.12/site-packages/torch/_functorch/apis.py:208\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/4A/probgraph/probgraph/.venv/lib/python3.12/site-packages/torch/_functorch/vmap.py:282\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    272\u001b[39m         func,\n\u001b[32m    273\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m         **kwargs,\n\u001b[32m    279\u001b[39m     )\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/4A/probgraph/probgraph/.venv/lib/python3.12/site-packages/torch/_functorch/vmap.py:432\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    429\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    430\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    431\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/4A/probgraph/probgraph/refs/former/former_code/stochastic_interpolants/interflow/stochastic_interpolant.py:640\u001b[39m, in \u001b[36mloss_per_sample_one_sided_b\u001b[39m\u001b[34m(b, x0, x1, t, interpolant)\u001b[39m\n\u001b[32m    638\u001b[39m dtIt        = interpolant.dtIt(t, x0, x1)\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# gamma_dot   = interpolant.gamma_dot(t)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m bt          = \u001b[43mb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m loss        = \u001b[32m0.5\u001b[39m*torch.sum(bt**\u001b[32m2\u001b[39m) - torch.sum((dtIt) * bt)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/4A/probgraph/probgraph/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/4A/probgraph/probgraph/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: VelocityNetwork.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # we set hyperparameters\n",
    "    base_lr = 1e-4\n",
    "    batch_size = 32\n",
    "    n_epochs = 5000\n",
    "    patch_size = 8\n",
    "    num_patches = 4\n",
    "    metrics_freq = 100\n",
    "    plot_freq = 500\n",
    "    \n",
    "    print(f\"\\nhyperparameters:\")\n",
    "    print(f\"  class: dog (cifar10 class 5)\")\n",
    "    print(f\"  batch_size: {batch_size}\")\n",
    "    print(f\"  learning_rate: {base_lr}\")\n",
    "    print(f\"  n_epochs: {n_epochs}\")\n",
    "    print(f\"  patch_size: {patch_size}\")\n",
    "    print(f\"  num_patches: {num_patches}\")\n",
    "    print(f\"\\nconditioning approach:\")\n",
    "    print(f\"  - networks conditioned on binary mask (visible=1, masked=0)\")\n",
    "    print(f\"  - visible pixels kept fixed during generation\")\n",
    "    print(f\"  - only masked regions are reconstructed\")\n",
    "    \n",
    "    # we define interpolant (one-sided linear interpolation)\n",
    "    path = 'one-sided-linear'\n",
    "    interpolant = stochastic_interpolant.Interpolant(path=path, gamma_type=None)\n",
    "    print(f\"\\nusing interpolant: {path}\")\n",
    "    \n",
    "    # we define loss functions\n",
    "    loss_fn_b = stochastic_interpolant.make_loss(\n",
    "        method='shared', interpolant=interpolant, loss_type='one-sided-b'\n",
    "    )\n",
    "    loss_fn_eta = stochastic_interpolant.make_loss(\n",
    "        method='shared', interpolant=interpolant, loss_type='one-sided-eta'\n",
    "    )\n",
    "    \n",
    "    # we create networks\n",
    "    print(\"\\ncreating u-net architectures with mask conditioning...\")\n",
    "    unet_b = UNetDenoiser(in_channels=5, out_channels=3, base_channels=64)  # we add mask channel\n",
    "    unet_eta = UNetDenoiser(in_channels=5, out_channels=3, base_channels=64)  # we add mask channel\n",
    "    \n",
    "    b = VelocityNetwork(unet_b).to(itf.util.get_torch_device())\n",
    "    eta = EtaNetwork(unet_eta).to(itf.util.get_torch_device())\n",
    "    \n",
    "    # we count parameters\n",
    "    n_params_b = sum(p.numel() for p in b.parameters() if p.requires_grad)\n",
    "    n_params_eta = sum(p.numel() for p in eta.parameters() if p.requires_grad)\n",
    "    print(f\"b network parameters: {n_params_b:,}\")\n",
    "    print(f\"eta network parameters: {n_params_eta:,}\")\n",
    "    \n",
    "    # we create optimizers and schedulers\n",
    "    opt_b = torch.optim.Adam(b.parameters(), lr=base_lr)\n",
    "    opt_eta = torch.optim.Adam(eta.parameters(), lr=base_lr)\n",
    "    sched_b = torch.optim.lr_scheduler.CosineAnnealingLR(opt_b, T_max=n_epochs, eta_min=base_lr*0.01)\n",
    "    sched_eta = torch.optim.lr_scheduler.CosineAnnealingLR(opt_eta, T_max=n_epochs, eta_min=base_lr*0.01)\n",
    "    \n",
    "    # we initialize data dictionary\n",
    "    data_dict = {\n",
    "        'losses': [],\n",
    "        'b_losses': [],\n",
    "        'eta_losses': [],\n",
    "        'b_grads': [],\n",
    "        'eta_grads': [],\n",
    "        'lrs': []\n",
    "    }\n",
    "    \n",
    "    # we train the model\n",
    "    print(\"\\nstarting training...\\n\")\n",
    "    counter = 1\n",
    "    for epoch in range(n_epochs):\n",
    "        loss, b_loss, eta_loss, b_grad, eta_grad = train_step(\n",
    "            batch_size, interpolant, opt_b, opt_eta, sched_b, sched_eta,\n",
    "            patch_size, num_patches\n",
    "        )\n",
    "        \n",
    "        # we log metrics\n",
    "        if (counter - 1) % metrics_freq == 0:\n",
    "            data_dict['losses'].append(grab(loss).item())\n",
    "            data_dict['b_losses'].append(grab(b_loss).item())\n",
    "            data_dict['eta_losses'].append(grab(eta_loss).item())\n",
    "            data_dict['b_grads'].append(grab(b_grad).item())\n",
    "            data_dict['eta_grads'].append(grab(eta_grad).item())\n",
    "            data_dict['lrs'].append(opt_b.param_groups[0]['lr'])\n",
    "            \n",
    "            print(f\"epoch {counter}: loss={grab(loss).item():.4f}, b_loss={grab(b_loss).item():.4f}, eta_loss={grab(eta_loss).item():.4f}\")\n",
    "        \n",
    "        # we make plots\n",
    "        if (counter - 1) % plot_freq == 0:\n",
    "            make_plots(b, eta, interpolant, counter, data_dict, patch_size, num_patches)\n",
    "            \n",
    "            # we save checkpoints\n",
    "            torch.save({\n",
    "                'epoch': counter,\n",
    "                'b_state_dict': b.state_dict(),\n",
    "                'eta_state_dict': eta.state_dict(),\n",
    "                'opt_b_state_dict': opt_b.state_dict(),\n",
    "                'opt_eta_state_dict': opt_eta.state_dict(),\n",
    "                'data_dict': data_dict,\n",
    "                'class': 'dog',\n",
    "                'class_id': 5\n",
    "            }, f'dog_checkpoint_epoch_{counter}.pt')\n",
    "            print(f\"saved checkpoint at epoch {counter}\")\n",
    "        \n",
    "        counter += 1\n",
    "    \n",
    "    print(\"\\ntraining complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
