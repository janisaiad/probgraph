{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601d7da0",
   "metadata": {},
   "source": [
    "cifar10 patch reconstruction using stochastic interpolants\n",
    "\n",
    "this implementation:\n",
    "- trains on dog class only (cifar10 class 5)\n",
    "- uses mask-conditioned u-net architecture\n",
    "- learns to reconstruct masked patches while keeping visible pixels fixed\n",
    "- conditions on mask during both training and generation\n",
    "- forces visible pixels to remain constant in output\n",
    "\n",
    "approach:\n",
    "  x0 = masked_image * mask + noise * (1-mask)  # we start with visible pixels + noise in masked regions\n",
    "  x1 = original_image                           # we target full image\n",
    "  model learns interpolant: x0 -> x1 conditioned on mask\n",
    "  during generation: output = model_output * (1-mask) + original * mask  # we force visible pixels fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e3b83a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available, setting default tensor residence to gpu\n",
      "cuda\n",
      "torch version: 2.9.0+cu128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/__init__.py:1275: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from typing import Tuple, Any\n",
    "\n",
    "import interflow as itf\n",
    "import interflow.stochastic_interpolant as stochastic_interpolant\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available, setting default tensor residence to gpu')\n",
    "    itf.util.set_torch_device('cuda')\n",
    "else:\n",
    "    print('no cuda device found')\n",
    "print(itf.util.get_torch_device())\n",
    "\n",
    "print(\"torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a94dfc",
   "metadata": {},
   "source": [
    "we define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88390200",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def grab(var):\n",
    "    \"\"\"we take a tensor off the gpu and convert it to a numpy array on the cpu\"\"\"\n",
    "    return var.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9d533",
   "metadata": {},
   "source": [
    "we load cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6ecb18",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "filtered dataset to class 'dog': 5000 images (out of 50000 total)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"../../data/cifar10\", train=True, \n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# we filter dataset to keep only dog class (class 5)\n",
    "# cifar10 classes: 0=airplane, 1=automobile, 2=bird, 3=cat, 4=deer, 5=dog, 6=frog, 7=horse, 8=ship, 9=truck\n",
    "target_class = 5  # we select dog class\n",
    "dog_indices = [i for i in range(len(trainset)) if trainset.targets[i] == target_class]\n",
    "print(f\"\\nfiltered dataset to class 'dog': {len(dog_indices)} images (out of {len(trainset)} total)\")\n",
    "\n",
    "# we create data iterator that only samples from dog images\n",
    "def get_cifar_batch(bs):\n",
    "    \"\"\"we get a batch of cifar10 dog images only\"\"\"\n",
    "    indices = torch.randint(0, len(dog_indices), (bs,))\n",
    "    imgs = torch.stack([trainset[dog_indices[i]][0] for i in indices])\n",
    "    return imgs.to(itf.util.get_torch_device())\n",
    "\n",
    "# we create masking function for patches\n",
    "def create_patch_mask(bs, patch_size=8, num_patches=4):\n",
    "    \"\"\"we create random patch masks, 1 for visible pixels, 0 for masked patches\"\"\"\n",
    "    mask = torch.ones(bs, 3, 32, 32)\n",
    "    for i in range(bs):\n",
    "        for _ in range(num_patches):\n",
    "            x = torch.randint(0, 32 - patch_size, (1,)).item()\n",
    "            y = torch.randint(0, 32 - patch_size, (1,)).item()\n",
    "            mask[i, :, x:x+patch_size, y:y+patch_size] = 0  # we mask the patch\n",
    "    return mask.to(itf.util.get_torch_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97549b9e",
   "metadata": {},
   "source": [
    "we define u-net style convolutional denoiser for image reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f038c539",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class UNetDenoiser(nn.Module):\n",
    "    \"\"\"we use u-net architecture with skip connections for image reconstruction, conditioned on mask\"\"\"\n",
    "    def __init__(self, in_channels=5, out_channels=3, base_channels=64):  # we add 1 channel for mask conditioning\n",
    "        super().__init__()\n",
    "        \n",
    "        # we define encoder (downsampling path)\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels*2, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*2, base_channels*2, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*2, base_channels*4, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*4, base_channels*4, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # we define bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*4, base_channels*8, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*8, base_channels*8, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # we define decoder (upsampling path) with skip connections\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*8, base_channels*4, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec3_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*8, base_channels*4, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*4, base_channels*4, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*4, base_channels*2, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec2_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*4, base_channels*2, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*2, base_channels*2, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*2, base_channels, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec1_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*2, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # we define final output layer\n",
    "        self.final = nn.Conv2d(base_channels, out_channels, 1)\n",
    "    \n",
    "    def forward(self, x_with_t):\n",
    "        \"\"\"we forward pass with skip connections\"\"\"\n",
    "        # we encode\n",
    "        e1 = self.enc1(x_with_t)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        \n",
    "        # we process bottleneck\n",
    "        b = self.bottleneck(e3)\n",
    "        \n",
    "        # we decode with skip connections\n",
    "        d3 = self.dec3(b)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3_conv(d3)\n",
    "        \n",
    "        d2 = self.dec2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2_conv(d2)\n",
    "        \n",
    "        d1 = self.dec1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1_conv(d1)\n",
    "        \n",
    "        # we output final reconstruction\n",
    "        out = self.final(d1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923a2a7",
   "metadata": {},
   "source": [
    "we define wrapper for eta network to match expected interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d5cf30",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EtaNetwork(nn.Module):\n",
    "    \"\"\"we wrap unet to accept concatenated [x, t, mask] input\"\"\"\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.mask = None  # we store mask for conditioning\n",
    "    \n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"we set the mask for conditioning\"\"\"\n",
    "        self.mask = mask\n",
    "    \n",
    "    def forward(self, xt, t=None):\n",
    "        \"\"\"we support xt with or without appended time; t may be provided separately\"\"\"\n",
    "        if xt.dim() == 1:\n",
    "            xt = xt.unsqueeze(0)  # we ensure batch dimension\n",
    "        bs = xt.shape[0]\n",
    "        feat_dim = xt.shape[1]\n",
    "        expected_x_dim = 3 * 32 * 32  # we expected flattened image dim\n",
    "        \n",
    "        if feat_dim == expected_x_dim + 1:\n",
    "            x_flat = xt[:, :-1]  # we split x and t when t is appended\n",
    "            t_vec = xt[:, -1:].to(device=xt.device, dtype=xt.dtype)  # we take t from input\n",
    "        elif feat_dim == expected_x_dim:\n",
    "            if t is None:\n",
    "                raise RuntimeError(\"t must be provided when xt has no appended time\")  # we enforce presence of t\n",
    "            if not isinstance(t, torch.Tensor):\n",
    "                t_vec = torch.tensor(t, device=xt.device, dtype=xt.dtype)  # we wrap python scalar/array\n",
    "            else:\n",
    "                t_vec = t.to(device=xt.device, dtype=xt.dtype)  # we move to same device/dtype\n",
    "            numel = t_vec.numel()\n",
    "            if numel == 1:\n",
    "                t_vec = t_vec.reshape(1, 1).expand(bs, 1)  # we broadcast single time to batch\n",
    "            elif numel == bs:\n",
    "                t_vec = t_vec.reshape(bs, 1)  # we ensure shape [bs,1]\n",
    "            else:\n",
    "                raise RuntimeError(f\"t has {numel} elements; expected 1 or {bs}\")  # we guard invalid sizes\n",
    "            x_flat = xt  # we use full vector as x\n",
    "        else:\n",
    "            raise RuntimeError(f\"unexpected feature dimension {feat_dim}; expected {expected_x_dim} or {expected_x_dim+1}\")  # we guard invalid dims\n",
    "        \n",
    "        x = x_flat.reshape(bs, 3, 32, 32)  # we reshape to image\n",
    "        t_channel = t_vec.view(bs, 1, 1, 1).expand(bs, 1, 32, 32)  # we spatially broadcast t\n",
    "        \n",
    "        # we add mask conditioning (use first channel of mask for simplicity)\n",
    "        if self.mask is not None:\n",
    "            mask_channel = self.mask[:bs, 0:1, :, :]  # we take first channel of mask [bs, 1, 32, 32]\n",
    "        else:\n",
    "            mask_channel = torch.ones(bs, 1, 32, 32, device=x.device)  # we default to all visible\n",
    "        \n",
    "        x_with_t_mask = torch.cat([x, t_channel, mask_channel], dim=1)  # we concatenate [x, t, mask]\n",
    "        \n",
    "        # we process through unet\n",
    "        out = self.unet(x_with_t_mask)\n",
    "        \n",
    "        # we flatten output\n",
    "        return out.reshape(bs, -1)\n",
    "\n",
    "# we define velocity field wrapper\n",
    "class VelocityNetwork(nn.Module):\n",
    "    \"\"\"we wrap unet for velocity field b, conditioned on mask\"\"\"\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.mask = None  # we store mask for conditioning\n",
    "    \n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"we set the mask for conditioning\"\"\"\n",
    "        self.mask = mask\n",
    "    \n",
    "    def forward(self, xt, t=None):\n",
    "        \"\"\"we support xt with or without appended time; t may be provided separately\"\"\"\n",
    "        if xt.dim() == 1:\n",
    "            xt = xt.unsqueeze(0)  # we ensure batch dimension\n",
    "        bs = xt.shape[0]\n",
    "        feat_dim = xt.shape[1]\n",
    "        expected_x_dim = 3 * 32 * 32  # we expected flattened image dim\n",
    "        \n",
    "        if feat_dim == expected_x_dim + 1:\n",
    "            x_flat = xt[:, :-1]  # we split x and t when t is appended\n",
    "            t_vec = xt[:, -1:].to(device=xt.device, dtype=xt.dtype)  # we take t from input\n",
    "        elif feat_dim == expected_x_dim:\n",
    "            if t is None:\n",
    "                raise RuntimeError(\"t must be provided when xt has no appended time\")  # we enforce presence of t\n",
    "            if not isinstance(t, torch.Tensor):\n",
    "                t_vec = torch.tensor(t, device=xt.device, dtype=xt.dtype)  # we wrap python scalar/array\n",
    "            else:\n",
    "                t_vec = t.to(device=xt.device, dtype=xt.dtype)  # we move to same device/dtype\n",
    "            numel = t_vec.numel()\n",
    "            if numel == 1:\n",
    "                t_vec = t_vec.reshape(1, 1).expand(bs, 1)  # we broadcast single time to batch\n",
    "            elif numel == bs:\n",
    "                t_vec = t_vec.reshape(bs, 1)  # we ensure shape [bs,1]\n",
    "            else:\n",
    "                raise RuntimeError(f\"t has {numel} elements; expected 1 or {bs}\")  # we guard invalid sizes\n",
    "            x_flat = xt  # we use full vector as x\n",
    "        else:\n",
    "            raise RuntimeError(f\"unexpected feature dimension {feat_dim}; expected {expected_x_dim} or {expected_x_dim+1}\")  # we guard invalid dims\n",
    "        \n",
    "        x = x_flat.reshape(bs, 3, 32, 32)  # we reshape to image\n",
    "        t_channel = t_vec.view(bs, 1, 1, 1).expand(bs, 1, 32, 32)  # we spatially broadcast t\n",
    "        \n",
    "        # we add mask conditioning (use first channel of mask for simplicity)\n",
    "        if self.mask is not None:\n",
    "            mask_channel = self.mask[:bs, 0:1, :, :]  # we take first channel of mask [bs, 1, 32, 32]\n",
    "        else:\n",
    "            mask_channel = torch.ones(bs, 1, 32, 32, device=x.device)  # we default to all visible\n",
    "        \n",
    "        x_with_t_mask = torch.cat([x, t_channel, mask_channel], dim=1)  # we concatenate [x, t, mask]\n",
    "        \n",
    "        # we process through unet\n",
    "        out = self.unet(x_with_t_mask)\n",
    "        \n",
    "        # we flatten output\n",
    "        return out.reshape(bs, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54d3f9",
   "metadata": {},
   "source": [
    "we define training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585e952a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    bs: int,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    opt_b: Any,\n",
    "    opt_eta: Any,\n",
    "    sched_b: Any,\n",
    "    sched_eta: Any,\n",
    "    patch_size: int,\n",
    "    num_patches: int,\n",
    "    mask_loss_weight: float = 10.0\n",
    "):\n",
    "    \"\"\"we take a single step of optimization on the training set\"\"\"\n",
    "    opt_b.zero_grad()\n",
    "    opt_eta.zero_grad()\n",
    "    \n",
    "    # we construct batch of real images\n",
    "    x1s_img = get_cifar_batch(bs)  # we get [bs, 3, 32, 32]\n",
    "    \n",
    "    # we create masks\n",
    "    masks = create_patch_mask(bs, patch_size=patch_size, num_patches=num_patches)  # we get [bs, 3, 32, 32]\n",
    "    \n",
    "    # we create masked images + noise in masked regions as starting point\n",
    "    noise = torch.randn_like(x1s_img) * (1 - masks)  # we add noise only in masked regions\n",
    "    x0s_img = x1s_img * masks + noise  # we combine masked image and noise\n",
    "    \n",
    "    # we flatten for interpolant\n",
    "    x0s = x0s_img.reshape(bs, -1)  # we flatten to [bs, 3072]\n",
    "    x1s = x1s_img.reshape(bs, -1)  # we flatten to [bs, 3072]\n",
    "    masks_flat = masks.reshape(bs, -1)  # we flatten mask too\n",
    "    \n",
    "    # we sample random times\n",
    "    ts = torch.rand(size=(bs,)).to(itf.util.get_torch_device())\n",
    "    \n",
    "    # we set masks for conditioning in both networks\n",
    "    b.set_mask(masks)\n",
    "    eta.set_mask(masks)\n",
    "    \n",
    "    # we compute the losses\n",
    "    loss_start = time.perf_counter()\n",
    "    loss_b_full = loss_fn_b(b, x0s, x1s, ts, interpolant)\n",
    "    loss_eta_full = loss_fn_eta(eta, x0s, x1s, ts, interpolant)\n",
    "    \n",
    "    # we weight the loss to focus on masked regions (multiply by mask_loss_weight for masked pixels)\n",
    "    # we compute per-pixel weight: visible pixels get weight 1.0, masked pixels get weight mask_loss_weight\n",
    "    loss_weights = masks_flat + mask_loss_weight * (1 - masks_flat)  # we create per-pixel weights\n",
    "    \n",
    "    # we apply weighted loss (approximation: we multiply total loss by average weight)\n",
    "    avg_weight = loss_weights.mean()\n",
    "    loss_b = loss_b_full * avg_weight\n",
    "    loss_eta = loss_eta_full * avg_weight\n",
    "    \n",
    "    loss_val = loss_b + loss_eta\n",
    "    loss_end = time.perf_counter()\n",
    "    \n",
    "    # we compute the gradient\n",
    "    backprop_start = time.perf_counter()\n",
    "    loss_b.backward()\n",
    "    loss_eta.backward()\n",
    "    b_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(b.parameters(), float('inf'))])\n",
    "    eta_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(eta.parameters(), float('inf'))])\n",
    "    backprop_end = time.perf_counter()\n",
    "    \n",
    "    # we perform the update\n",
    "    update_start = time.perf_counter()\n",
    "    opt_b.step()\n",
    "    opt_eta.step()\n",
    "    sched_b.step()\n",
    "    sched_eta.step()\n",
    "    update_end = time.perf_counter()\n",
    "    \n",
    "    if counter < 5:\n",
    "        print(f'[loss: {loss_end - loss_start:.4f}s], [backprop: {backprop_end-backprop_start:.4f}s], [update: {update_end-update_start:.4f}s]')\n",
    "    \n",
    "    return loss_val.detach(), loss_b.detach(), loss_eta.detach(), b_grad.detach(), eta_grad.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb0992",
   "metadata": {},
   "source": [
    "we define visualization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a106ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_plots(\n",
    "    b: torch.nn.Module,\n",
    "    eta: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    counter: int,\n",
    "    data_dict: dict,\n",
    "    patch_size: int,\n",
    "    num_patches: int\n",
    "):\n",
    "    \"\"\"we make plots to visualize reconstruction results\"\"\"\n",
    "    print(f\"\\nepoch: {counter}\")\n",
    "    \n",
    "    # we get a batch for visualization\n",
    "    vis_bs = 8\n",
    "    x1s_img = get_cifar_batch(vis_bs)\n",
    "    masks = create_patch_mask(vis_bs, patch_size=patch_size, num_patches=num_patches)\n",
    "    \n",
    "    # we create masked images\n",
    "    noise = torch.randn_like(x1s_img) * (1 - masks)\n",
    "    x0s_img = x1s_img * masks + noise\n",
    "    \n",
    "    # we reconstruct using probability flow\n",
    "    x0s = x0s_img.reshape(vis_bs, -1)\n",
    "    x1s = x1s_img.reshape(vis_bs, -1)\n",
    "    \n",
    "    # we use simple forward integration\n",
    "    with torch.no_grad():\n",
    "        # we set masks for conditioning during generation\n",
    "        b.set_mask(masks)\n",
    "        eta.set_mask(masks)\n",
    "        \n",
    "        s = stochastic_interpolant.SFromEta(eta, interpolant.a)\n",
    "        pflow = stochastic_interpolant.PFlowIntegrator(\n",
    "            b=b, method='dopri5', interpolant=interpolant, n_step=10\n",
    "        )\n",
    "        xfs_pflow, _ = pflow.rollout(x0s)\n",
    "        xf_pflow_raw = xfs_pflow[-1].reshape(vis_bs, 3, 32, 32)\n",
    "        \n",
    "        # we force visible pixels to remain fixed (only reconstruct masked regions)\n",
    "        xf_pflow = xf_pflow_raw * (1 - masks) + x1s_img * masks  # we keep original pixels where mask=1\n",
    "    \n",
    "    # we plot results\n",
    "    fig, axes = plt.subplots(3, vis_bs, figsize=(vis_bs*2, 6))\n",
    "    \n",
    "    for i in range(vis_bs):\n",
    "        # we denormalize images for visualization\n",
    "        def denorm(img):\n",
    "            return img * 0.5 + 0.5\n",
    "        \n",
    "        # we show original image\n",
    "        axes[0, i].imshow(np.transpose(grab(denorm(x1s_img[i])), (1, 2, 0)))\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('original', fontsize=10)\n",
    "        \n",
    "        # we show masked image\n",
    "        axes[1, i].imshow(np.transpose(grab(denorm(x0s_img[i])), (1, 2, 0)))\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('masked', fontsize=10)\n",
    "        \n",
    "        # we show reconstructed image\n",
    "        axes[2, i].imshow(np.transpose(grab(denorm(xf_pflow[i])), (1, 2, 0)))\n",
    "        axes[2, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[2, i].set_title('reconstructed', fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f'dog patch reconstruction - epoch {counter}', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'dog_reconstruction_epoch_{counter}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # we plot training curves\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    epochs = np.arange(len(data_dict['losses'])) * metrics_freq\n",
    "    \n",
    "    # we plot losses\n",
    "    axes[0].plot(epochs, data_dict['losses'], label='total loss', linewidth=2)\n",
    "    axes[0].plot(epochs, data_dict['b_losses'], label='b loss', alpha=0.7)\n",
    "    axes[0].plot(epochs, data_dict['eta_losses'], label='eta loss', alpha=0.7)\n",
    "    axes[0].set_xlabel('epoch')\n",
    "    axes[0].set_ylabel('loss')\n",
    "    axes[0].set_title('training loss (dog class)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # we plot gradients\n",
    "    axes[1].plot(epochs, data_dict['b_grads'], label='b grad norm', linewidth=2)\n",
    "    axes[1].plot(epochs, data_dict['eta_grads'], label='eta grad norm', linewidth=2)\n",
    "    axes[1].set_xlabel('epoch')\n",
    "    axes[1].set_ylabel('gradient norm')\n",
    "    axes[1].set_title('gradient norms')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # we plot learning rates\n",
    "    axes[2].plot(epochs, data_dict['lrs'], label='learning rate', linewidth=2)\n",
    "    axes[2].set_xlabel('epoch')\n",
    "    axes[2].set_ylabel('learning rate')\n",
    "    axes[2].set_title('learning rate schedule')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'dog_training_curves_epoch_{counter}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037762ad",
   "metadata": {},
   "source": [
    "we define main training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37475ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hyperparameters:\n",
      "  class: dog (cifar10 class 5)\n",
      "  batch_size: 32\n",
      "  learning_rate: 0.0001\n",
      "  n_epochs: 5000\n",
      "  patch_size: 8\n",
      "  num_patches: 4\n",
      "\n",
      "conditioning approach:\n",
      "  - networks conditioned on binary mask (visible=1, masked=0)\n",
      "  - visible pixels kept fixed during generation\n",
      "  - only masked regions are reconstructed\n",
      "\n",
      "using interpolant: one-sided-linear\n",
      "\n",
      "creating u-net architectures with mask conditioning...\n",
      "b network parameters: 8,565,315\n",
      "eta network parameters: 8,565,315\n",
      "\n",
      "starting training...\n",
      "\n",
      "[loss: 0.1305s], [backprop: 0.0530s], [update: 0.0221s]\n",
      "epoch 1: loss=2747.9614, b_loss=469.9946, eta_loss=2277.9668\n",
      "\n",
      "epoch: 1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 19.53 GiB of which 704.00 KiB is free. Process 185113 has 370.00 MiB memory in use. Including non-PyTorch memory, this process has 19.02 GiB memory in use. Of the allocated memory 18.80 GiB is allocated by PyTorch, and 25.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# we make plots\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (counter - \u001b[32m1\u001b[39m) % plot_freq == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[43mmake_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# we save checkpoints\u001b[39;00m\n\u001b[32m     91\u001b[39m     torch.save({\n\u001b[32m     92\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m: counter,\n\u001b[32m     93\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mb_state_dict\u001b[39m\u001b[33m'\u001b[39m: b.state_dict(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mclass_id\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m5\u001b[39m\n\u001b[32m    100\u001b[39m     }, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdog_checkpoint_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mmake_plots\u001b[39m\u001b[34m(b, eta, interpolant, counter, data_dict, patch_size, num_patches)\u001b[39m\n\u001b[32m     32\u001b[39m s = stochastic_interpolant.SFromEta(eta, interpolant.a)\n\u001b[32m     33\u001b[39m pflow = stochastic_interpolant.PFlowIntegrator(\n\u001b[32m     34\u001b[39m     b=b, method=\u001b[33m'\u001b[39m\u001b[33mdopri5\u001b[39m\u001b[33m'\u001b[39m, interpolant=interpolant, n_step=\u001b[32m10\u001b[39m\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m xfs_pflow, _ = \u001b[43mpflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m xf_pflow_raw = xfs_pflow[-\u001b[32m1\u001b[39m].reshape(vis_bs, \u001b[32m3\u001b[39m, \u001b[32m32\u001b[39m, \u001b[32m32\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# we force visible pixels to remain fixed (only reconstruct masked regions)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/refs/former/former_code/stochastic_interpolants/interflow/stochastic_interpolant.py:249\u001b[39m, in \u001b[36mPFlowIntegrator.rollout\u001b[39m\u001b[34m(self, x0, reverse)\u001b[39m\n\u001b[32m    246\u001b[39m     integration_times = torch.linspace(\u001b[38;5;28mself\u001b[39m.start, \u001b[38;5;28mself\u001b[39m.end, \u001b[38;5;28mself\u001b[39m.n_step).to(x0)\n\u001b[32m    247\u001b[39m dlogp = torch.zeros(x0.shape[\u001b[32m0\u001b[39m]).to(x0)\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m state = \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdlogp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintegration_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43matol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m x, dlogp = state\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, dlogp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torchdiffeq/_impl/adjoint.py:206\u001b[39m, in \u001b[36modeint_adjoint\u001b[39m\u001b[34m(func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, adjoint_params)\u001b[39m\n\u001b[32m    203\u001b[39m state_norm = options[\u001b[33m\"\u001b[39m\u001b[33mnorm\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    204\u001b[39m handle_adjoint_norm_(adjoint_options, shapes, state_norm)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m ans = \u001b[43mOdeintAdjointMethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjoint_rtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjoint_atol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m                                \u001b[49m\u001b[43madjoint_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjoint_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madjoint_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    210\u001b[39m     solution = ans\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/autograd/function.py:581\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    579\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    580\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    586\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    587\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    588\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    589\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torchdiffeq/_impl/adjoint.py:24\u001b[39m, in \u001b[36mOdeintAdjointMethod.forward\u001b[39m\u001b[34m(ctx, shapes, func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, t_requires_grad, *adjoint_params)\u001b[39m\n\u001b[32m     21\u001b[39m ctx.event_mode = event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     ans = \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevent_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     27\u001b[39m         y = ans\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torchdiffeq/_impl/odeint.py:80\u001b[39m, in \u001b[36modeint\u001b[39m\u001b[34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[39m\n\u001b[32m     77\u001b[39m solver = SOLVERS[method](func=func, y0=y0, rtol=rtol, atol=atol, **options)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     solution = \u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     82\u001b[39m     event_t, solution = solver.integrate_until_event(t[\u001b[32m0\u001b[39m], event_fn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torchdiffeq/_impl/solvers.py:32\u001b[39m, in \u001b[36mAdaptiveStepsizeODESolver.integrate\u001b[39m\u001b[34m(self, t)\u001b[39m\n\u001b[32m     30\u001b[39m solution[\u001b[32m0\u001b[39m] = \u001b[38;5;28mself\u001b[39m.y0\n\u001b[32m     31\u001b[39m t = t.to(\u001b[38;5;28mself\u001b[39m.dtype)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_before_integrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t)):\n\u001b[32m     34\u001b[39m     solution[i] = \u001b[38;5;28mself\u001b[39m._advance(t[i])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torchdiffeq/_impl/rk_common.py:213\u001b[39m, in \u001b[36mRKAdaptiveStepsizeODESolver._before_integrate\u001b[39m\u001b[34m(self, t)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_before_integrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, t):\n\u001b[32m    212\u001b[39m     t0 = t[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     f0 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.first_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    215\u001b[39m         first_step = _select_initial_step(\u001b[38;5;28mself\u001b[39m.func, t[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.y0, \u001b[38;5;28mself\u001b[39m.order - \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.rtol, \u001b[38;5;28mself\u001b[39m.atol,\n\u001b[32m    216\u001b[39m                                           \u001b[38;5;28mself\u001b[39m.norm, f0=f0)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torchdiffeq/_impl/misc.py:197\u001b[39m, in \u001b[36m_PerturbFunc.forward\u001b[39m\u001b[34m(self, t, y, perturb)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torchdiffeq/_impl/misc.py:197\u001b[39m, in \u001b[36m_PerturbFunc.forward\u001b[39m\u001b[34m(self, t, y, perturb)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torchdiffeq/_impl/misc.py:144\u001b[39m, in \u001b[36m_TupleFunc.forward\u001b[39m\u001b[34m(self, t, y)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_flat_to_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([f_.reshape(-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f_ \u001b[38;5;129;01min\u001b[39;00m f])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/refs/former/former_code/stochastic_interpolants/interflow/stochastic_interpolant.py:171\u001b[39m, in \u001b[36mPFlowRHS.forward\u001b[39m\u001b[34m(self, t, states)\u001b[39m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.rhs(x, t), torch.zeros(x.shape[\u001b[32m0\u001b[39m]).to(x))\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.rhs(x, t), -\u001b[43mcompute_div\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/refs/former/former_code/stochastic_interpolants/interflow/stochastic_interpolant.py:32\u001b[39m, in \u001b[36mcompute_div\u001b[39m\u001b[34m(f, x, t)\u001b[39m\n\u001b[32m     29\u001b[39m     divergence = \u001b[32m0.0\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x.shape[\u001b[32m1\u001b[39m]):\n\u001b[32m     31\u001b[39m         divergence += \\\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m                 \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mf_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][:, i]\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m divergence.view(bs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:503\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    499\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    500\u001b[39m         grad_outputs_\n\u001b[32m    501\u001b[39m     )\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    514\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    515\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    516\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 19.53 GiB of which 704.00 KiB is free. Process 185113 has 370.00 MiB memory in use. Including non-PyTorch memory, this process has 19.02 GiB memory in use. Of the allocated memory 18.80 GiB is allocated by PyTorch, and 25.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # we set hyperparameters\n",
    "    base_lr = 1e-4\n",
    "    batch_size = 32\n",
    "    n_epochs = 5000\n",
    "    patch_size = 8\n",
    "    num_patches = 4\n",
    "    metrics_freq = 100\n",
    "    plot_freq = 500\n",
    "    \n",
    "    print(f\"\\nhyperparameters:\")\n",
    "    print(f\"  class: dog (cifar10 class 5)\")\n",
    "    print(f\"  batch_size: {batch_size}\")\n",
    "    print(f\"  learning_rate: {base_lr}\")\n",
    "    print(f\"  n_epochs: {n_epochs}\")\n",
    "    print(f\"  patch_size: {patch_size}\")\n",
    "    print(f\"  num_patches: {num_patches}\")\n",
    "    print(f\"\\nconditioning approach:\")\n",
    "    print(f\"  - networks conditioned on binary mask (visible=1, masked=0)\")\n",
    "    print(f\"  - visible pixels kept fixed during generation\")\n",
    "    print(f\"  - only masked regions are reconstructed\")\n",
    "    \n",
    "    # we define interpolant (one-sided linear interpolation)\n",
    "    path = 'one-sided-linear'\n",
    "    interpolant = stochastic_interpolant.Interpolant(path=path, gamma_type=None)\n",
    "    print(f\"\\nusing interpolant: {path}\")\n",
    "    \n",
    "    # we define loss functions\n",
    "    loss_fn_b = stochastic_interpolant.make_loss(\n",
    "        method='shared', interpolant=interpolant, loss_type='one-sided-b'\n",
    "    )\n",
    "    loss_fn_eta = stochastic_interpolant.make_loss(\n",
    "        method='shared', interpolant=interpolant, loss_type='one-sided-eta'\n",
    "    )\n",
    "    \n",
    "    # we create networks\n",
    "    print(\"\\ncreating u-net architectures with mask conditioning...\")\n",
    "    unet_b = UNetDenoiser(in_channels=5, out_channels=3, base_channels=64)  # we add mask channel\n",
    "    unet_eta = UNetDenoiser(in_channels=5, out_channels=3, base_channels=64)  # we add mask channel\n",
    "    \n",
    "    b = VelocityNetwork(unet_b).to(itf.util.get_torch_device())\n",
    "    eta = EtaNetwork(unet_eta).to(itf.util.get_torch_device())\n",
    "    \n",
    "    # we count parameters\n",
    "    n_params_b = sum(p.numel() for p in b.parameters() if p.requires_grad)\n",
    "    n_params_eta = sum(p.numel() for p in eta.parameters() if p.requires_grad)\n",
    "    print(f\"b network parameters: {n_params_b:,}\")\n",
    "    print(f\"eta network parameters: {n_params_eta:,}\")\n",
    "    \n",
    "    # we create optimizers and schedulers\n",
    "    opt_b = torch.optim.Adam(b.parameters(), lr=base_lr)\n",
    "    opt_eta = torch.optim.Adam(eta.parameters(), lr=base_lr)\n",
    "    sched_b = torch.optim.lr_scheduler.CosineAnnealingLR(opt_b, T_max=n_epochs, eta_min=base_lr*0.01)\n",
    "    sched_eta = torch.optim.lr_scheduler.CosineAnnealingLR(opt_eta, T_max=n_epochs, eta_min=base_lr*0.01)\n",
    "    \n",
    "    # we initialize data dictionary\n",
    "    data_dict = {\n",
    "        'losses': [],\n",
    "        'b_losses': [],\n",
    "        'eta_losses': [],\n",
    "        'b_grads': [],\n",
    "        'eta_grads': [],\n",
    "        'lrs': []\n",
    "    }\n",
    "    \n",
    "    # we train the model\n",
    "    print(\"\\nstarting training...\\n\")\n",
    "    counter = 1\n",
    "    for epoch in range(n_epochs):\n",
    "        loss, b_loss, eta_loss, b_grad, eta_grad = train_step(\n",
    "            batch_size, interpolant, opt_b, opt_eta, sched_b, sched_eta,\n",
    "            patch_size, num_patches\n",
    "        )\n",
    "        \n",
    "        # we log metrics\n",
    "        if (counter - 1) % metrics_freq == 0:\n",
    "            data_dict['losses'].append(grab(loss).item())\n",
    "            data_dict['b_losses'].append(grab(b_loss).item())\n",
    "            data_dict['eta_losses'].append(grab(eta_loss).item())\n",
    "            data_dict['b_grads'].append(grab(b_grad).item())\n",
    "            data_dict['eta_grads'].append(grab(eta_grad).item())\n",
    "            data_dict['lrs'].append(opt_b.param_groups[0]['lr'])\n",
    "            \n",
    "            print(f\"epoch {counter}: loss={grab(loss).item():.4f}, b_loss={grab(b_loss).item():.4f}, eta_loss={grab(eta_loss).item():.4f}\")\n",
    "        \n",
    "        # we make plots\n",
    "        if (counter - 1) % plot_freq == 0:\n",
    "            make_plots(b, eta, interpolant, counter, data_dict, patch_size, num_patches)\n",
    "            \n",
    "            # we save checkpoints\n",
    "            torch.save({\n",
    "                'epoch': counter,\n",
    "                'b_state_dict': b.state_dict(),\n",
    "                'eta_state_dict': eta.state_dict(),\n",
    "                'opt_b_state_dict': opt_b.state_dict(),\n",
    "                'opt_eta_state_dict': opt_eta.state_dict(),\n",
    "                'data_dict': data_dict,\n",
    "                'class': 'dog',\n",
    "                'class_id': 5\n",
    "            }, f'dog_checkpoint_epoch_{counter}.pt')\n",
    "            print(f\"saved checkpoint at epoch {counter}\")\n",
    "        \n",
    "        counter += 1\n",
    "    \n",
    "    print(\"\\ntraining complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
