{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../')\n",
    "from typing import Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bceaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import interflow as itf\n",
    "import interflow.stochastic_interpolant as stochastic_interpolant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a73612",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('cuda available, setting default tensor residence to gpu')\n",
    "    itf.util.set_torch_device('cuda')\n",
    "else:\n",
    "    print('no cuda device found')\n",
    "print(itf.util.get_torch_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32739cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160bcd1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define utility functions\n",
    "def grab(var):\n",
    "    \"\"\"we take a tensor off the gpu and convert it to a numpy array on the cpu\"\"\"\n",
    "    return var.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff19754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load cifar10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root=\"../../data/cifar10\", train=True, \n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acef3a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we filter dataset to keep only dog class (class 5)\n",
    "# cifar10 classes: 0=airplane, 1=automobile, 2=bird, 3=cat, 4=deer, 5=dog, 6=frog, 7=horse, 8=ship, 9=truck\n",
    "target_class = 5  # we select dog class\n",
    "dog_indices = [i for i in range(len(trainset)) if trainset.targets[i] == target_class]\n",
    "print(f\"\\nfiltered dataset to class 'dog': {len(dog_indices)} images (out of {len(trainset)} total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94d44a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we create data iterator that only samples from dog images\n",
    "def get_cifar_batch(bs):\n",
    "    \"\"\"we get a batch of cifar10 dog images only\"\"\"\n",
    "    indices = torch.randint(0, len(dog_indices), (bs,))\n",
    "    imgs = torch.stack([trainset[dog_indices[i]][0] for i in indices])\n",
    "    return imgs.to(itf.util.get_torch_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8c67c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we create masking function for patches\n",
    "def create_patch_mask(bs, patch_size=8, num_patches=4):\n",
    "    \"\"\"we create random patch masks, 1 for visible pixels, 0 for masked patches\"\"\"\n",
    "    mask = torch.ones(bs, 3, 32, 32)\n",
    "    for i in range(bs):\n",
    "        for _ in range(num_patches):\n",
    "            x = torch.randint(0, 32 - patch_size, (1,)).item()\n",
    "            y = torch.randint(0, 32 - patch_size, (1,)).item()\n",
    "            mask[i, :, x:x+patch_size, y:y+patch_size] = 0  # we mask the patch\n",
    "    return mask.to(itf.util.get_torch_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5fa3a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define u-net style convolutional denoiser for image reconstruction\n",
    "class UNetDenoiser(nn.Module):\n",
    "    \"\"\"we use u-net architecture with skip connections for image reconstruction\"\"\"\n",
    "    def __init__(self, in_channels=4, out_channels=3, base_channels=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # we define encoder (downsampling path)\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels*2, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*2, base_channels*2, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*2, base_channels*4, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*4, base_channels*4, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # we define bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*4, base_channels*8, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*8, base_channels*8, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # we define decoder (upsampling path) with skip connections\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*8, base_channels*4, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec3_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*8, base_channels*4, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*4, base_channels*4, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*4, base_channels*2, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec2_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*4, base_channels*2, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels*2, base_channels*2, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*2, base_channels, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec1_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels*2, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # we define final output layer\n",
    "        self.final = nn.Conv2d(base_channels, out_channels, 1)\n",
    "    \n",
    "    def forward(self, x_with_t):\n",
    "        \"\"\"we forward pass with skip connections\"\"\"\n",
    "        # we encode\n",
    "        e1 = self.enc1(x_with_t)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        \n",
    "        # we process bottleneck\n",
    "        b = self.bottleneck(e3)\n",
    "        \n",
    "        # we decode with skip connections\n",
    "        d3 = self.dec3(b)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3_conv(d3)\n",
    "        \n",
    "        d2 = self.dec2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2_conv(d2)\n",
    "        \n",
    "        d1 = self.dec1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1_conv(d1)\n",
    "        \n",
    "        # we output final reconstruction\n",
    "        out = self.final(d1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4754178",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define wrapper for eta network to match expected interface\n",
    "class EtaNetwork(nn.Module):\n",
    "    \"\"\"we wrap unet to accept separate (x, t) inputs\"\"\"\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet  # we store underlying unet\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"we expect x of shape [bs, 3*32*32] and scalar or batched t\"\"\"\n",
    "        bs = x.shape[0]  # we get batch size\n",
    "        x_img = x.reshape(bs, 3, 32, 32)  # we reshape flat image to 4d tensor\n",
    "        \n",
    "        if not isinstance(t, torch.Tensor):  # we convert non-tensor time to tensor\n",
    "            t_tensor = torch.tensor(t, device=x.device, dtype=x.dtype)\n",
    "        else:\n",
    "            t_tensor = t.to(x.device)  # we move time tensor to same device as x\n",
    "        \n",
    "        if t_tensor.dim() == 0:  # we handle scalar time\n",
    "            t_batch = t_tensor.repeat(bs).unsqueeze(1)  # we repeat scalar t over batch\n",
    "        elif t_tensor.dim() == 1:  # we handle vector time\n",
    "            if t_tensor.shape[0] == 1 and bs > 1:  # we broadcast single time value to batch\n",
    "                t_batch = t_tensor.repeat(bs).unsqueeze(1)\n",
    "            else:\n",
    "                t_batch = t_tensor.unsqueeze(1)  # we add feature dimension\n",
    "        elif t_tensor.dim() == 2:  # we handle matrix-shaped time\n",
    "            if t_tensor.shape[0] == 1 and bs > 1:  # we broadcast row to batch\n",
    "                t_batch = t_tensor.repeat(bs, 1)\n",
    "            else:\n",
    "                t_batch = t_tensor  # we assume proper shape [bs, 1]\n",
    "        else:\n",
    "            raise ValueError(\"we expect time tensor of dimension at most 2\")\n",
    "        \n",
    "        t_channel = t_batch.view(bs, 1, 1, 1).expand(bs, 1, 32, 32)  # we broadcast time as extra channel\n",
    "        x_with_t = torch.cat([x_img, t_channel], dim=1)  # we concatenate time as 4th channel\n",
    "        \n",
    "        out = self.unet(x_with_t)  # we run unet\n",
    "        return out.reshape(bs, -1)  # we flatten output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a8fe5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define velocity field wrapper\n",
    "class VelocityNetwork(nn.Module):\n",
    "    \"\"\"we wrap unet for velocity field b with separate (x, t) inputs\"\"\"\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet  # we store underlying unet\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"we expect x of shape [bs, 3*32*32] and scalar or batched t\"\"\"\n",
    "        bs = x.shape[0]  # we get batch size\n",
    "        x_img = x.reshape(bs, 3, 32, 32)  # we reshape flat image to 4d tensor\n",
    "        \n",
    "        if not isinstance(t, torch.Tensor):  # we convert non-tensor time to tensor\n",
    "            t_tensor = torch.tensor(t, device=x.device, dtype=x.dtype)\n",
    "        else:\n",
    "            t_tensor = t.to(x.device)  # we move time tensor to same device as x\n",
    "        \n",
    "        if t_tensor.dim() == 0:  # we handle scalar time\n",
    "            t_batch = t_tensor.repeat(bs).unsqueeze(1)  # we repeat scalar t over batch\n",
    "        elif t_tensor.dim() == 1:  # we handle vector time\n",
    "            if t_tensor.shape[0] == 1 and bs > 1:  # we broadcast single time value to batch\n",
    "                t_batch = t_tensor.repeat(bs).unsqueeze(1)\n",
    "            else:\n",
    "                t_batch = t_tensor.unsqueeze(1)  # we add feature dimension\n",
    "        elif t_tensor.dim() == 2:  # we handle matrix-shaped time\n",
    "            if t_tensor.shape[0] == 1 and bs > 1:  # we broadcast row to batch\n",
    "                t_batch = t_tensor.repeat(bs, 1)\n",
    "            else:\n",
    "                t_batch = t_tensor  # we assume proper shape [bs, 1]\n",
    "        else:\n",
    "            raise ValueError(\"we expect time tensor of dimension at most 2\")\n",
    "        \n",
    "        t_channel = t_batch.view(bs, 1, 1, 1).expand(bs, 1, 32, 32)  # we broadcast time as extra channel\n",
    "        x_with_t = torch.cat([x_img, t_channel], dim=1)  # we concatenate time as 4th channel\n",
    "        \n",
    "        out = self.unet(x_with_t)  # we run unet\n",
    "        return out.reshape(bs, -1)  # we flatten output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9cc93",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define training step function\n",
    "def train_step(\n",
    "    bs: int,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    opt_b: Any,\n",
    "    opt_eta: Any,\n",
    "    sched_b: Any,\n",
    "    sched_eta: Any,\n",
    "    patch_size: int,\n",
    "    num_patches: int,\n",
    "    mask_loss_weight: float = 10.0\n",
    "):\n",
    "    \"\"\"we take a single step of optimization on the training set\"\"\"\n",
    "    opt_b.zero_grad()\n",
    "    opt_eta.zero_grad()\n",
    "    \n",
    "    # we construct batch of real images\n",
    "    x1s_img = get_cifar_batch(bs)  # we get [bs, 3, 32, 32]\n",
    "    \n",
    "    # we create masks\n",
    "    masks = create_patch_mask(bs, patch_size=patch_size, num_patches=num_patches)  # we get [bs, 3, 32, 32]\n",
    "    \n",
    "    # we create masked images + noise in masked regions as starting point\n",
    "    noise = torch.randn_like(x1s_img) * (1 - masks)  # we add noise only in masked regions\n",
    "    x0s_img = x1s_img * masks + noise  # we combine masked image and noise\n",
    "    \n",
    "    # we flatten for interpolant\n",
    "    x0s = x0s_img.reshape(bs, -1)  # we flatten to [bs, 3072]\n",
    "    x1s = x1s_img.reshape(bs, -1)  # we flatten to [bs, 3072]\n",
    "    masks_flat = masks.reshape(bs, -1)  # we flatten mask too\n",
    "    \n",
    "    # we sample random times\n",
    "    ts = torch.rand(size=(bs,)).to(itf.util.get_torch_device())\n",
    "    \n",
    "    # we compute the losses\n",
    "    loss_start = time.perf_counter()\n",
    "    loss_b = loss_fn_b(b, x0s, x1s, ts, interpolant)\n",
    "    loss_eta = loss_fn_eta(eta, x0s, x1s, ts, interpolant)\n",
    "    \n",
    "    # we add extra weight to loss on masked regions\n",
    "    loss_val = loss_b + loss_eta\n",
    "    loss_end = time.perf_counter()\n",
    "    \n",
    "    # we compute the gradient\n",
    "    backprop_start = time.perf_counter()\n",
    "    loss_b.backward()\n",
    "    loss_eta.backward()\n",
    "    b_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(b.parameters(), float('inf'))])\n",
    "    eta_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(eta.parameters(), float('inf'))])\n",
    "    backprop_end = time.perf_counter()\n",
    "    \n",
    "    # we perform the update\n",
    "    update_start = time.perf_counter()\n",
    "    opt_b.step()\n",
    "    opt_eta.step()\n",
    "    sched_b.step()\n",
    "    sched_eta.step()\n",
    "    update_end = time.perf_counter()\n",
    "    \n",
    "    if counter < 5:\n",
    "        print(f'[loss: {loss_end - loss_start:.4f}s], [backprop: {backprop_end-backprop_start:.4f}s], [update: {update_end-update_start:.4f}s]')\n",
    "    \n",
    "    return loss_val.detach(), loss_b.detach(), loss_eta.detach(), b_grad.detach(), eta_grad.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc6e74",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define visualization function\n",
    "def make_plots(\n",
    "    b: torch.nn.Module,\n",
    "    eta: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    counter: int,\n",
    "    data_dict: dict,\n",
    "    patch_size: int,\n",
    "    num_patches: int\n",
    "):\n",
    "    \"\"\"we make plots to visualize reconstruction results\"\"\"\n",
    "    print(f\"\\nepoch: {counter}\")\n",
    "    \n",
    "    # we get a batch for visualization\n",
    "    vis_bs = 8\n",
    "    x1s_img = get_cifar_batch(vis_bs)\n",
    "    masks = create_patch_mask(vis_bs, patch_size=patch_size, num_patches=num_patches)\n",
    "    \n",
    "    # we create masked images\n",
    "    noise = torch.randn_like(x1s_img) * (1 - masks)\n",
    "    x0s_img = x1s_img * masks + noise\n",
    "    \n",
    "    # we reconstruct using probability flow\n",
    "    x0s = x0s_img.reshape(vis_bs, -1)\n",
    "    x1s = x1s_img.reshape(vis_bs, -1)\n",
    "    \n",
    "    # we use simple forward integration\n",
    "    with torch.no_grad():\n",
    "        s = stochastic_interpolant.SFromEta(eta, interpolant.a)\n",
    "        pflow = stochastic_interpolant.PFlowIntegrator(\n",
    "            b=b, method='dopri5', interpolant=interpolant, n_step=10\n",
    "        )\n",
    "        xfs_pflow, _ = pflow.rollout(x0s)\n",
    "        xf_pflow = xfs_pflow[-1].reshape(vis_bs, 3, 32, 32)\n",
    "    \n",
    "    # we plot results\n",
    "    fig, axes = plt.subplots(3, vis_bs, figsize=(vis_bs*2, 6))\n",
    "    \n",
    "    for i in range(vis_bs):\n",
    "        # we denormalize images for visualization\n",
    "        def denorm(img):\n",
    "            return img * 0.5 + 0.5\n",
    "        \n",
    "        # we show original image\n",
    "        axes[0, i].imshow(np.transpose(grab(denorm(x1s_img[i])), (1, 2, 0)))\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('original', fontsize=10)\n",
    "        \n",
    "        # we show masked image\n",
    "        axes[1, i].imshow(np.transpose(grab(denorm(x0s_img[i])), (1, 2, 0)))\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('masked', fontsize=10)\n",
    "        \n",
    "        # we show reconstructed image\n",
    "        axes[2, i].imshow(np.transpose(grab(denorm(xf_pflow[i])), (1, 2, 0)))\n",
    "        axes[2, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[2, i].set_title('reconstructed', fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f'dog patch reconstruction - epoch {counter}', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    results_dir = os.path.join(\"vresults\", \"cifarpatchedinterpolants\")  # we define directory for saving figures\n",
    "    os.makedirs(results_dir, exist_ok=True)  # we ensure results directory exists\n",
    "    recon_path = os.path.join(results_dir, f\"dog_reconstruction_epoch_{counter}.png\")  # we build reconstruction path\n",
    "    plt.savefig(recon_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # we plot training curves\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    epochs = np.arange(len(data_dict[\"losses\"])) * metrics_freq\n",
    "    \n",
    "    # we plot losses\n",
    "    axes[0].plot(epochs, data_dict[\"losses\"], label=\"total loss\", linewidth=2)\n",
    "    axes[0].plot(epochs, data_dict[\"b_losses\"], label=\"b loss\", alpha=0.7)\n",
    "    axes[0].plot(epochs, data_dict[\"eta_losses\"], label=\"eta loss\", alpha=0.7)\n",
    "    axes[0].set_xlabel(\"epoch\")\n",
    "    axes[0].set_ylabel(\"loss\")\n",
    "    axes[0].set_title(\"training loss (dog class)\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # we plot gradients\n",
    "    axes[1].plot(epochs, data_dict[\"b_grads\"], label=\"b grad norm\", linewidth=2)\n",
    "    axes[1].plot(epochs, data_dict[\"eta_grads\"], label=\"eta grad norm\", linewidth=2)\n",
    "    axes[1].set_xlabel(\"epoch\")\n",
    "    axes[1].set_ylabel(\"gradient norm\")\n",
    "    axes[1].set_title(\"gradient norms\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # we plot learning rates\n",
    "    axes[2].plot(epochs, data_dict[\"lrs\"], label=\"learning rate\", linewidth=2)\n",
    "    axes[2].set_xlabel(\"epoch\")\n",
    "    axes[2].set_ylabel(\"learning rate\")\n",
    "    axes[2].set_title(\"learning rate schedule\")\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    path_name = interpolant.path  # we get interpolant path name\n",
    "    gamma_name = getattr(interpolant, \"gamma_type\", \"none\")  # we get gamma schedule name\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    results_dir = os.path.join(\"vresults\", \"cifarpatchedinterpolants\")  # we define directory for saving curves\n",
    "    os.makedirs(results_dir, exist_ok=True)  # we ensure results directory exists\n",
    "    curves_path = os.path.join(results_dir, f\"dog_training_curves_{path_name}_{gamma_name}_epoch_{counter}.png\")  # we build curves path\n",
    "    plt.savefig(curves_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabea9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define main training setup\n",
    "if __name__ == \"__main__\":\n",
    "    # we set hyperparameters\n",
    "    base_lr = 1e-4  # we set base learning rate\n",
    "    batch_size = 32  # we set batch size\n",
    "    n_epochs = 5000  # we set number of epochs\n",
    "    patch_size = 8  # we set patch size\n",
    "    num_patches = 4  # we set number of patches\n",
    "    metrics_freq = 100  # we set metrics logging frequency\n",
    "    plot_freq = 500  # we set plotting frequency\n",
    "    \n",
    "    print(\"\\nhyperparameters:\")  # we print header for hyperparameters\n",
    "    print(f\"  class: dog (cifar10 class 5)\")  # we print target class\n",
    "    print(f\"  batch_size: {batch_size}\")  # we print batch size\n",
    "    print(f\"  learning_rate: {base_lr}\")  # we print base learning rate\n",
    "    print(f\"  n_epochs: {n_epochs}\")  # we print number of epochs\n",
    "    print(f\"  patch_size: {patch_size}\")  # we print patch size\n",
    "    print(f\"  num_patches: {num_patches}\")  # we print number of patches\n",
    "    \n",
    "    paths = [\"linear\", \"trig\", \"encoding-decoding\"]  # we define two-sided interpolant paths\n",
    "    gamma_types = [\"bsquared\", \"sinesquared\", \"sigmoid\"]  # we define non-gaussian gamma schedules\n",
    "    \n",
    "    for path in paths:\n",
    "        for gamma_type in gamma_types:\n",
    "            print(f\"\\nusing interpolant: path={path}, gamma_type={gamma_type}\")  # we log current interpolant config\n",
    "            \n",
    "            interpolant = stochastic_interpolant.Interpolant(path=path, gamma_type=gamma_type)  # we create interpolant\n",
    "            interpolant.gamma_type = gamma_type  # we store gamma type on interpolant for logging\n",
    "            \n",
    "            # we define loss functions for two-sided stochastic interpolant\n",
    "            loss_fn_b = stochastic_interpolant.make_loss(\n",
    "                method=\"shared\", interpolant=interpolant, loss_type=\"b\"\n",
    "            )\n",
    "            loss_fn_eta = stochastic_interpolant.make_loss(\n",
    "                method=\"shared\", interpolant=interpolant, loss_type=\"eta\"\n",
    "            )\n",
    "            \n",
    "            # we create networks\n",
    "            print(\"\\ncreating u-net architectures...\")  # we notify network instantiation\n",
    "            unet_b = UNetDenoiser(in_channels=4, out_channels=3, base_channels=64)  # we create u-net for b\n",
    "            unet_eta = UNetDenoiser(in_channels=4, out_channels=3, base_channels=64)  # we create u-net for eta\n",
    "            \n",
    "            b = VelocityNetwork(unet_b).to(itf.util.get_torch_device())  # we move b network to device\n",
    "            eta = EtaNetwork(unet_eta).to(itf.util.get_torch_device())  # we move eta network to device\n",
    "            \n",
    "            # we count parameters\n",
    "            n_params_b = sum(p.numel() for p in b.parameters() if p.requires_grad)  # we count trainable params of b\n",
    "            n_params_eta = sum(p.numel() for p in eta.parameters() if p.requires_grad)  # we count trainable params of eta\n",
    "            print(f\"b network parameters: {n_params_b:,}\")  # we print number of parameters of b\n",
    "            print(f\"eta network parameters: {n_params_eta:,}\")  # we print number of parameters of eta\n",
    "            \n",
    "            # we create optimizers and schedulers\n",
    "            opt_b = torch.optim.Adam(b.parameters(), lr=base_lr)  # we create optimizer for b\n",
    "            opt_eta = torch.optim.Adam(eta.parameters(), lr=base_lr)  # we create optimizer for eta\n",
    "            sched_b = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                opt_b, T_max=n_epochs, eta_min=base_lr * 0.01\n",
    "            )  # we create lr scheduler for b\n",
    "            sched_eta = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                opt_eta, T_max=n_epochs, eta_min=base_lr * 0.01\n",
    "            )  # we create lr scheduler for eta\n",
    "            \n",
    "            # we initialize data dictionary\n",
    "            data_dict = {\n",
    "                \"losses\": [],\n",
    "                \"b_losses\": [],\n",
    "                \"eta_losses\": [],\n",
    "                \"b_grads\": [],\n",
    "                \"eta_grads\": [],\n",
    "                \"lrs\": [],\n",
    "            }  # we store training metrics\n",
    "            \n",
    "            # we train the model for this configuration\n",
    "            print(\"\\nstarting training...\\n\")  # we announce training start\n",
    "            counter = 1  # we initialize iteration counter\n",
    "            for epoch in range(n_epochs):\n",
    "                loss, b_loss, eta_loss, b_grad, eta_grad = train_step(\n",
    "                    batch_size, interpolant, opt_b, opt_eta, sched_b, sched_eta, patch_size, num_patches\n",
    "                )  # we perform one training step\n",
    "                \n",
    "                # we log metrics\n",
    "                if (counter - 1) % metrics_freq == 0:\n",
    "                    data_dict[\"losses\"].append(grab(loss).item())  # we log total loss\n",
    "                    data_dict[\"b_losses\"].append(grab(b_loss).item())  # we log b loss\n",
    "                    data_dict[\"eta_losses\"].append(grab(eta_loss).item())  # we log eta loss\n",
    "                    data_dict[\"b_grads\"].append(grab(b_grad).item())  # we log gradient norm of b\n",
    "                    data_dict[\"eta_grads\"].append(grab(eta_grad).item())  # we log gradient norm of eta\n",
    "                    data_dict[\"lrs\"].append(opt_b.param_groups[0][\"lr\"])  # we log learning rate\n",
    "                    \n",
    "                    print(\n",
    "                        f\"[path={path} | gamma_type={gamma_type}] epoch {counter}: \"\n",
    "                        f\"loss={grab(loss).item():.4f}, \"\n",
    "                        f\"b_loss={grab(b_loss).item():.4f}, \"\n",
    "                        f\"eta_loss={grab(eta_loss).item():.4f}\"\n",
    "                    )  # we print training status\n",
    "                \n",
    "                # we make plots and save checkpoints\n",
    "                if (counter - 1) % plot_freq == 0:\n",
    "                    make_plots(b, eta, interpolant, counter, data_dict, patch_size, num_patches)  # we visualize results\n",
    "                    \n",
    "                    results_dir = os.path.join(\"vresults\", \"cifarpatchedinterpolants\")  # we define directory for checkpoints\n",
    "                    os.makedirs(results_dir, exist_ok=True)  # we ensure results directory exists\n",
    "                    ckpt_name = os.path.join(\n",
    "                        results_dir,\n",
    "                        f\"dog_checkpoint_{path}_{gamma_type}_epoch_{counter}.pt\",\n",
    "                    )  # we build checkpoint filename\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"epoch\": counter,\n",
    "                            \"b_state_dict\": b.state_dict(),\n",
    "                            \"eta_state_dict\": eta.state_dict(),\n",
    "                            \"opt_b_state_dict\": opt_b.state_dict(),\n",
    "                            \"opt_eta_state_dict\": opt_eta.state_dict(),\n",
    "                            \"data_dict\": data_dict,\n",
    "                            \"class\": \"dog\",\n",
    "                            \"class_id\": 5,\n",
    "                            \"path\": path,\n",
    "                            \"gamma_type\": gamma_type,\n",
    "                        },\n",
    "                        ckpt_name,\n",
    "                    )  # we save checkpoint\n",
    "                    print(\n",
    "                        f\"saved checkpoint at epoch {counter} for path={path}, gamma_type={gamma_type}\"\n",
    "                    )  # we log checkpoint saving\n",
    "                \n",
    "                counter += 1  # we increment iteration counter\n",
    "            \n",
    "            print(f\"\\ntraining complete for path={path}, gamma_type={gamma_type}\\n\")  # we mark configuration as finished"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
