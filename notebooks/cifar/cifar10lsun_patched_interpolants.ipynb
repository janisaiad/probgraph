{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8e8e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../')\n",
    "from typing import Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e07761a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/janis.aiad/probgraph/refs/former/former_code/stochastic_interpolants/interflow/stochastic_interpolant.py:82: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import interflow as itf\n",
    "import interflow.stochastic_interpolant as stochastic_interpolant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "214276fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available, setting default tensor residence to gpu\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/janis.aiad/probgraph/.venv/lib/python3.12/site-packages/torch/__init__.py:1275: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('cuda available, setting default tensor residence to gpu')\n",
    "    itf.util.set_torch_device('cuda')\n",
    "else:\n",
    "    print('no cuda device found')\n",
    "print(itf.util.get_torch_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9106360",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.9.1+cu128\n"
     ]
    }
   ],
   "source": [
    "print(\"torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dca218b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define utility functions\n",
    "def grab(var):\n",
    "    \"\"\"we take a tensor off the gpu and convert it to a numpy array on the cpu\"\"\"\n",
    "    return var.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b91f899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load lsun dataset (bedroom class)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # we resize lsun images to 32x32\n",
    "    transforms.ToTensor(),  # we convert images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # we normalize channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e78b1b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "../../data/lsun/bedroom_train_lmdb: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainset = \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLSUN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../../data/lsun\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we set lsun root directory\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbedroom_train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we use bedroom training subset\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we apply preprocessing transforms\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mloaded lsun bedroom_train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(trainset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# we log lsun subset size\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.12/site-packages/torchvision/datasets/lsun.py:86\u001b[39m, in \u001b[36mLSUN.__init__\u001b[39m\u001b[34m(self, root, classes, transform, target_transform)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m.dbs = []\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classes:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28mself\u001b[39m.dbs.append(\u001b[43mLSUNClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_lmdb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.indices = []\n\u001b[32m     89\u001b[39m count = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Data/janis.aiad/probgraph/.venv/lib/python3.12/site-packages/torchvision/datasets/lsun.py:23\u001b[39m, in \u001b[36mLSUNClass.__init__\u001b[39m\u001b[34m(self, root, transform, target_transform)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlmdb\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(root, transform=transform, target_transform=target_transform)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28mself\u001b[39m.env = \u001b[43mlmdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_readers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreadonly\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreadahead\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeminit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.begin(write=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m txn:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mself\u001b[39m.length = txn.stat()[\u001b[33m\"\u001b[39m\u001b[33mentries\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mError\u001b[39m: ../../data/lsun/bedroom_train_lmdb: No such file or directory"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.LSUN(\n",
    "    root=\"../../data/lsun\",  # we set lsun root directory\n",
    "    classes=[\"bedroom_train\"],  # we use bedroom training subset\n",
    "    transform=transform  # we apply preprocessing transforms\n",
    ")\n",
    "print(f\"\\nloaded lsun bedroom_train: {len(trainset)} images\")  # we log lsun subset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af07e60",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we create data iterator that only samples from lsun bedroom images\n",
    "def get_cifar_batch(bs):\n",
    "    \"\"\"we get a batch of lsun bedroom images only\"\"\"\n",
    "    indices = torch.randint(0, len(trainset), (bs,))  # we sample random indices\n",
    "    imgs = torch.stack([trainset[i][0] for i in indices])  # we stack selected images\n",
    "    return imgs.to(itf.util.get_torch_device())  # we move batch to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a38cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we create masking function for patches\n",
    "def create_patch_mask(bs, patch_size=8, num_patches=4):\n",
    "    \"\"\"we create random patch masks, 1 for visible pixels, 0 for masked patches\"\"\"\n",
    "    mask = torch.ones(bs, 3, 32, 32)\n",
    "    for i in range(bs):\n",
    "        for _ in range(num_patches):\n",
    "            x = torch.randint(0, 32 - patch_size, (1,)).item()\n",
    "            y = torch.randint(0, 32 - patch_size, (1,)).item()\n",
    "            mask[i, :, x:x+patch_size, y:y+patch_size] = 0  # we mask the patch\n",
    "    return mask.to(itf.util.get_torch_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"we build sinusoidal positional embeddings for scalar time\"\"\"  # we describe time embedding\n",
    "    def __init__(self, embedding_dim: int, max_period: float = 10000.0) -> None:\n",
    "        super().__init__()  # we call parent constructor\n",
    "        self.embedding_dim = embedding_dim  # we store embedding dimension\n",
    "        self.max_period = max_period  # we store maximum period\n",
    "    \n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"we map scalar timesteps to sinusoidal embeddings\"\"\"  # we describe forward\n",
    "        if not isinstance(t, torch.Tensor):  # we convert non-tensor to tensor\n",
    "            t_tensor = torch.tensor(t, dtype=torch.float32)  # we build tensor from scalar or array\n",
    "        else:\n",
    "            t_tensor = t.float()  # we cast to float\n",
    "        if t_tensor.dim() == 0:  # we handle scalar time\n",
    "            t_tensor = t_tensor[None]  # we add batch dimension\n",
    "        if t_tensor.dim() == 2 and t_tensor.shape[1] == 1:  # we squeeze singleton feature dimension\n",
    "            t_tensor = t_tensor[:, 0]  # we reduce to shape [bs]\n",
    "        device = t_tensor.device  # we get device\n",
    "        half_dim = self.embedding_dim // 2  # we compute half dimension\n",
    "        if half_dim < 1:  # we guard against invalid configuration\n",
    "            raise ValueError(\"we expect embedding_dim to be at least 2\")  # we raise error for tiny dims\n",
    "        exponent = -torch.log(torch.tensor(self.max_period, device=device)) / float(half_dim - 1)  # we compute exponent\n",
    "        freqs = torch.exp(torch.arange(half_dim, device=device, dtype=torch.float32) * exponent)  # we build frequencies\n",
    "        args = t_tensor.view(-1, 1) * freqs.view(1, -1)  # we build outer product of time and frequencies\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)  # we concatenate sin and cos\n",
    "        if self.embedding_dim % 2 == 1:  # we pad for odd dimension\n",
    "            emb = torch.nn.functional.pad(emb, (0, 1))  # we add one zero dimension\n",
    "        return emb  # we return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cca770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeResBlock(nn.Module):\n",
    "    \"\"\"we define residual block modulated by time embedding\"\"\"  # we describe residual block\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_dim: int, stride: int = 1) -> None:\n",
    "        super().__init__()  # we call parent constructor\n",
    "        self.in_channels = in_channels  # we store input channels\n",
    "        self.out_channels = out_channels  # we store output channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)  # we define first conv\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)  # we define first group norm\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)  # we define second conv\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)  # we define second group norm\n",
    "        self.act = nn.ReLU()  # we define activation\n",
    "        self.time_mlp = nn.Linear(time_dim, out_channels)  # we define linear layer for time embedding\n",
    "        if stride != 1 or in_channels != out_channels:  # we check if skip connection must project\n",
    "            self.skip = nn.Conv2d(in_channels, out_channels, 1, stride=stride)  # we define skip projection\n",
    "        else:\n",
    "            self.skip = nn.Identity()  # we define identity skip\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"we apply residual block given time embedding\"\"\"  # we describe forward\n",
    "        h = self.conv1(x)  # we apply first convolution\n",
    "        h = self.norm1(h)  # we normalize\n",
    "        h = self.act(h)  # we activate\n",
    "        time_out = self.time_mlp(t_emb).view(t_emb.shape[0], self.out_channels, 1, 1)  # we project time embedding\n",
    "        h = h + time_out  # we inject time information\n",
    "        h = self.conv2(h)  # we apply second convolution\n",
    "        h = self.norm2(h)  # we normalize\n",
    "        h = self.act(h)  # we activate\n",
    "        return h + self.skip(x)  # we add residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e891627",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define u-net style convolutional denoiser for image reconstruction\n",
    "class UNetDenoiser(nn.Module):\n",
    "    \"\"\"we use u-net architecture with time-conditioned residual blocks for image reconstruction\"\"\"  # we describe unet\n",
    "    def __init__(self, in_channels: int = 3, out_channels: int = 3, base_channels: int = 64, time_dim: int = 256) -> None:\n",
    "        super().__init__()  # we call parent constructor\n",
    "        \n",
    "        self.time_embedding = SinusoidalTimeEmbedding(time_dim)  # we build sinusoidal time encoder\n",
    "        self.time_mlp = nn.Sequential(  # we refine time embedding\n",
    "            nn.Linear(time_dim, time_dim),  # we apply linear projection\n",
    "            nn.SiLU(),  # we apply nonlinearity\n",
    "            nn.Linear(time_dim, time_dim),  # we apply second projection\n",
    "        )  # we define time mlp\n",
    "        \n",
    "        # we define encoder (downsampling path) with time-conditioned residual blocks\n",
    "        self.enc1 = TimeResBlock(in_channels, base_channels, time_dim, stride=1)  # we define first encoder block\n",
    "        self.enc2 = TimeResBlock(base_channels, base_channels * 2, time_dim, stride=2)  # we define second encoder block\n",
    "        self.enc3 = TimeResBlock(base_channels * 2, base_channels * 4, time_dim, stride=2)  # we define third encoder block\n",
    "        \n",
    "        # we define bottleneck\n",
    "        self.bottleneck = TimeResBlock(base_channels * 4, base_channels * 8, time_dim, stride=2)  # we define bottleneck block\n",
    "        \n",
    "        # we define decoder (upsampling path) with skip connections and time-conditioned residual blocks\n",
    "        self.dec3_up = nn.ConvTranspose2d(\n",
    "            base_channels * 8, base_channels * 4, 3, stride=2, padding=1, output_padding=1\n",
    "        )  # we upsample from bottleneck\n",
    "        self.dec3_block = TimeResBlock(base_channels * 8, base_channels * 4, time_dim, stride=1)  # we refine with skip from enc3\n",
    "        \n",
    "        self.dec2_up = nn.ConvTranspose2d(\n",
    "            base_channels * 4, base_channels * 2, 3, stride=2, padding=1, output_padding=1\n",
    "        )  # we upsample\n",
    "        self.dec2_block = TimeResBlock(base_channels * 4, base_channels * 2, time_dim, stride=1)  # we refine with skip from enc2\n",
    "        \n",
    "        self.dec1_up = nn.ConvTranspose2d(\n",
    "            base_channels * 2, base_channels, 3, stride=2, padding=1, output_padding=1\n",
    "        )  # we upsample\n",
    "        self.dec1_block = TimeResBlock(base_channels * 2, base_channels, time_dim, stride=1)  # we refine with skip from enc1\n",
    "        \n",
    "        # we define final output layer\n",
    "        self.final = nn.Conv2d(base_channels, out_channels, 1)  # we project to rgb\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"we forward pass with time-conditioned residual blocks\"\"\"  # we describe forward\n",
    "        t_emb = self.time_embedding(t.to(x.device))  # we build sinusoidal time embedding\n",
    "        t_emb = self.time_mlp(t_emb)  # we refine time embedding\n",
    "        \n",
    "        # we encode\n",
    "        e1 = self.enc1(x, t_emb)  # we apply first encoder block\n",
    "        e2 = self.enc2(e1, t_emb)  # we apply second encoder block\n",
    "        e3 = self.enc3(e2, t_emb)  # we apply third encoder block\n",
    "        \n",
    "        # we process bottleneck\n",
    "        b = self.bottleneck(e3, t_emb)  # we apply bottleneck block\n",
    "        \n",
    "        # we decode with skip connections\n",
    "        d3 = self.dec3_up(b)  # we upsample from bottleneck\n",
    "        d3 = torch.cat([d3, e3], dim=1)  # we concatenate skip connection from encoder level 3\n",
    "        d3 = self.dec3_block(d3, t_emb)  # we apply decoder block 3\n",
    "        \n",
    "        d2 = self.dec2_up(d3)  # we upsample\n",
    "        d2 = torch.cat([d2, e2], dim=1)  # we concatenate skip from encoder level 2\n",
    "        d2 = self.dec2_block(d2, t_emb)  # we apply decoder block 2\n",
    "        \n",
    "        d1 = self.dec1_up(d2)  # we upsample\n",
    "        d1 = torch.cat([d1, e1], dim=1)  # we concatenate skip from encoder level 1\n",
    "        d1 = self.dec1_block(d1, t_emb)  # we apply decoder block 1\n",
    "        \n",
    "        # we output final reconstruction\n",
    "        out = self.final(d1)  # we map to rgb\n",
    "        return out  # we return reconstructed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875b9c6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define wrapper for eta network to match expected interface\n",
    "class EtaNetwork(nn.Module):\n",
    "    \"\"\"we wrap unet to accept flattened x and scalar time inputs\"\"\"  # we describe eta wrapper\n",
    "    def __init__(self, unet: nn.Module) -> None:\n",
    "        super().__init__()  # we call parent constructor\n",
    "        self.unet = unet  # we store underlying unet\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"we expect x of shape [bs, 3*32*32] and scalar or batched t\"\"\"  # we describe forward\n",
    "        bs = x.shape[0]  # we get batch size\n",
    "        x_img = x.reshape(bs, 3, 32, 32)  # we reshape flat image to 4d tensor\n",
    "        out = self.unet(x_img, t)  # we run time-conditioned unet\n",
    "        return out.reshape(bs, -1)  # we flatten output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3dc637",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define velocity field wrapper\n",
    "class VelocityNetwork(nn.Module):\n",
    "    \"\"\"we wrap unet for velocity field b with flattened x and scalar time inputs\"\"\"  # we describe velocity wrapper\n",
    "    def __init__(self, unet: nn.Module) -> None:\n",
    "        super().__init__()  # we call parent constructor\n",
    "        self.unet = unet  # we store underlying unet\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"we expect x of shape [bs, 3*32*32] and scalar or batched t\"\"\"  # we describe forward\n",
    "        bs = x.shape[0]  # we get batch size\n",
    "        x_img = x.reshape(bs, 3, 32, 32)  # we reshape flat image to 4d tensor\n",
    "        out = self.unet(x_img, t)  # we run time-conditioned unet\n",
    "        return out.reshape(bs, -1)  # we flatten output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fbb332",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define training step function\n",
    "def train_step(\n",
    "    bs: int,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    opt_b: Any,\n",
    "    opt_eta: Any,\n",
    "    sched_b: Any,\n",
    "    sched_eta: Any,\n",
    "    patch_size: int,\n",
    "    num_patches: int,\n",
    "    mask_loss_weight: float = 10.0\n",
    "):\n",
    "    \"\"\"we take a single step of optimization on the training set\"\"\"\n",
    "    opt_b.zero_grad()\n",
    "    opt_eta.zero_grad()\n",
    "    \n",
    "    # we construct batch of real images\n",
    "    x1s_img = get_cifar_batch(bs)  # we get [bs, 3, 32, 32]\n",
    "    \n",
    "    # we create masks\n",
    "    masks = create_patch_mask(bs, patch_size=patch_size, num_patches=num_patches)  # we get [bs, 3, 32, 32]\n",
    "    \n",
    "    # we create masked images + noise in masked regions as starting point\n",
    "    noise = torch.randn_like(x1s_img) * (1 - masks)  # we add noise only in masked regions\n",
    "    x0s_img = x1s_img * masks + noise  # we combine masked image and noise\n",
    "    \n",
    "    # we flatten for interpolant\n",
    "    x0s = x0s_img.reshape(bs, -1)  # we flatten to [bs, 3072]\n",
    "    x1s = x1s_img.reshape(bs, -1)  # we flatten to [bs, 3072]\n",
    "    masks_flat = masks.reshape(bs, -1)  # we flatten mask too\n",
    "    \n",
    "    # we sample random times\n",
    "    ts = torch.rand(size=(bs,)).to(itf.util.get_torch_device())\n",
    "    \n",
    "    # we compute the losses\n",
    "    loss_start = time.perf_counter()\n",
    "    loss_b = loss_fn_b(b, x0s, x1s, ts, interpolant)\n",
    "    loss_eta = loss_fn_eta(eta, x0s, x1s, ts, interpolant)\n",
    "    \n",
    "    # we add extra weight to loss on masked regions\n",
    "    loss_val = loss_b + loss_eta\n",
    "    loss_end = time.perf_counter()\n",
    "    \n",
    "    # we compute the gradient\n",
    "    backprop_start = time.perf_counter()\n",
    "    loss_b.backward()\n",
    "    loss_eta.backward()\n",
    "    b_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(b.parameters(), float('inf'))])\n",
    "    eta_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(eta.parameters(), float('inf'))])\n",
    "    backprop_end = time.perf_counter()\n",
    "    \n",
    "    # we perform the update\n",
    "    update_start = time.perf_counter()\n",
    "    opt_b.step()\n",
    "    opt_eta.step()\n",
    "    sched_b.step()\n",
    "    sched_eta.step()\n",
    "    update_end = time.perf_counter()\n",
    "    \n",
    "    if counter < 5:\n",
    "        print(f'[loss: {loss_end - loss_start:.4f}s], [backprop: {backprop_end-backprop_start:.4f}s], [update: {update_end-update_start:.4f}s]')\n",
    "    \n",
    "    return loss_val.detach(), loss_b.detach(), loss_eta.detach(), b_grad.detach(), eta_grad.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69501d2f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# we define visualization function\n",
    "def make_plots(\n",
    "    b: torch.nn.Module,\n",
    "    eta: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    counter: int,\n",
    "    data_dict: dict,\n",
    "    patch_size: int,\n",
    "    num_patches: int\n",
    "):\n",
    "    \"\"\"we make plots to visualize reconstruction results\"\"\"\n",
    "    print(f\"\\nepoch: {counter}\")\n",
    "    \n",
    "    # we get a batch for visualization\n",
    "    vis_bs = 8\n",
    "    x1s_img = get_cifar_batch(vis_bs)\n",
    "    masks = create_patch_mask(vis_bs, patch_size=patch_size, num_patches=num_patches)\n",
    "    \n",
    "    # we create masked images\n",
    "    noise = torch.randn_like(x1s_img) * (1 - masks)\n",
    "    x0s_img = x1s_img * masks + noise\n",
    "    \n",
    "    # we reconstruct using probability flow\n",
    "    x0s = x0s_img.reshape(vis_bs, -1)\n",
    "    x1s = x1s_img.reshape(vis_bs, -1)\n",
    "    \n",
    "    # we use simple forward integration\n",
    "    with torch.no_grad():\n",
    "        s = stochastic_interpolant.SFromEta(eta, interpolant.a)\n",
    "        pflow = stochastic_interpolant.PFlowIntegrator(\n",
    "            b=b, method='dopri5', interpolant=interpolant, n_step=10\n",
    "        )\n",
    "        xfs_pflow, _ = pflow.rollout(x0s)\n",
    "        xf_pflow = xfs_pflow[-1].reshape(vis_bs, 3, 32, 32)\n",
    "    \n",
    "    # we plot results\n",
    "    fig, axes = plt.subplots(3, vis_bs, figsize=(vis_bs*2, 6))\n",
    "    \n",
    "    for i in range(vis_bs):\n",
    "        # we denormalize images for visualization\n",
    "        def denorm(img):\n",
    "            return img * 0.5 + 0.5\n",
    "        \n",
    "        # we show original image\n",
    "        axes[0, i].imshow(np.transpose(grab(denorm(x1s_img[i])), (1, 2, 0)))\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('original', fontsize=10)\n",
    "        \n",
    "        # we show masked image\n",
    "        axes[1, i].imshow(np.transpose(grab(denorm(x0s_img[i])), (1, 2, 0)))\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('masked', fontsize=10)\n",
    "        \n",
    "        # we show reconstructed image\n",
    "        axes[2, i].imshow(np.transpose(grab(denorm(xf_pflow[i])), (1, 2, 0)))\n",
    "        axes[2, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[2, i].set_title('reconstructed', fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f'dog patch reconstruction - epoch {counter}', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    results_dir = os.path.join(\"vresults\", \"cifarpatchedinterpolants\")  # we define directory for saving figures\n",
    "    os.makedirs(results_dir, exist_ok=True)  # we ensure results directory exists\n",
    "    recon_path = os.path.join(results_dir, f\"dog_reconstruction_epoch_{counter}.png\")  # we build reconstruction path\n",
    "    plt.savefig(recon_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # we plot training curves\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    epochs = np.arange(len(data_dict[\"losses\"])) * metrics_freq\n",
    "    \n",
    "    # we plot losses\n",
    "    axes[0].plot(epochs, data_dict[\"losses\"], label=\"total loss\", linewidth=2)\n",
    "    axes[0].plot(epochs, data_dict[\"b_losses\"], label=\"b loss\", alpha=0.7)\n",
    "    axes[0].plot(epochs, data_dict[\"eta_losses\"], label=\"eta loss\", alpha=0.7)\n",
    "    axes[0].set_xlabel(\"epoch\")\n",
    "    axes[0].set_ylabel(\"loss\")\n",
    "    axes[0].set_title(\"training loss (dog class)\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # we plot gradients\n",
    "    axes[1].plot(epochs, data_dict[\"b_grads\"], label=\"b grad norm\", linewidth=2)\n",
    "    axes[1].plot(epochs, data_dict[\"eta_grads\"], label=\"eta grad norm\", linewidth=2)\n",
    "    axes[1].set_xlabel(\"epoch\")\n",
    "    axes[1].set_ylabel(\"gradient norm\")\n",
    "    axes[1].set_title(\"gradient norms\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # we plot learning rates\n",
    "    axes[2].plot(epochs, data_dict[\"lrs\"], label=\"learning rate\", linewidth=2)\n",
    "    axes[2].set_xlabel(\"epoch\")\n",
    "    axes[2].set_ylabel(\"learning rate\")\n",
    "    axes[2].set_title(\"learning rate schedule\")\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    path_name = interpolant.path  # we get interpolant path name\n",
    "    gamma_name = getattr(interpolant, \"gamma_type\", \"none\")  # we get gamma schedule name\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    results_dir = os.path.join(\"vresults\", \"cifarpatchedinterpolants\")  # we define directory for saving curves\n",
    "    os.makedirs(results_dir, exist_ok=True)  # we ensure results directory exists\n",
    "    curves_path = os.path.join(results_dir, f\"dog_training_curves_{path_name}_{gamma_name}_epoch_{counter}.png\")  # we build curves path\n",
    "    plt.savefig(curves_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23209404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define main training setup\n",
    "if __name__ == \"__main__\":\n",
    "    # we set hyperparameters\n",
    "    base_lr = 1e-4  # we set base learning rate\n",
    "    batch_size = 32  # we set batch size\n",
    "    n_epochs = 5000  # we set number of epochs\n",
    "    patch_size = 8  # we set patch size\n",
    "    num_patches = 4  # we set number of patches\n",
    "    metrics_freq = 100  # we set metrics logging frequency\n",
    "    plot_freq = 500  # we set plotting frequency\n",
    "    \n",
    "    print(\"\\nhyperparameters:\")  # we print header for hyperparameters\n",
    "    print(f\"  class: dog (cifar10 class 5)\")  # we print target class\n",
    "    print(f\"  batch_size: {batch_size}\")  # we print batch size\n",
    "    print(f\"  learning_rate: {base_lr}\")  # we print base learning rate\n",
    "    print(f\"  n_epochs: {n_epochs}\")  # we print number of epochs\n",
    "    print(f\"  patch_size: {patch_size}\")  # we print patch size\n",
    "    print(f\"  num_patches: {num_patches}\")  # we print number of patches\n",
    "    \n",
    "    paths = [\"linear\", \"trig\", \"encoding-decoding\"]  # we define two-sided interpolant paths\n",
    "    gamma_types = [\"bsquared\", \"sinesquared\", \"sigmoid\"]  # we define non-gaussian gamma schedules\n",
    "    \n",
    "    for path in paths:\n",
    "        for gamma_type in gamma_types:\n",
    "            print(f\"\\nusing interpolant: path={path}, gamma_type={gamma_type}\")  # we log current interpolant config\n",
    "            \n",
    "            interpolant = stochastic_interpolant.Interpolant(path=path, gamma_type=gamma_type)  # we create interpolant\n",
    "            interpolant.gamma_type = gamma_type  # we store gamma type on interpolant for logging\n",
    "            \n",
    "            # we define loss functions for two-sided stochastic interpolant\n",
    "            loss_fn_b = stochastic_interpolant.make_loss(\n",
    "                method=\"shared\", interpolant=interpolant, loss_type=\"b\"\n",
    "            )\n",
    "            loss_fn_eta = stochastic_interpolant.make_loss(\n",
    "                method=\"shared\", interpolant=interpolant, loss_type=\"eta\"\n",
    "            )\n",
    "            \n",
    "            # we create networks\n",
    "            print(\"\\ncreating u-net architectures...\")  # we notify network instantiation\n",
    "            unet_b = UNetDenoiser(in_channels=3, out_channels=3, base_channels=64)  # we create time-conditioned unet for b\n",
    "            unet_eta = UNetDenoiser(in_channels=3, out_channels=3, base_channels=64)  # we create time-conditioned unet for eta\n",
    "            \n",
    "            b = VelocityNetwork(unet_b).to(itf.util.get_torch_device())  # we move b network to device\n",
    "            eta = EtaNetwork(unet_eta).to(itf.util.get_torch_device())  # we move eta network to device\n",
    "            \n",
    "            # we count parameters\n",
    "            n_params_b = sum(p.numel() for p in b.parameters() if p.requires_grad)  # we count trainable params of b\n",
    "            n_params_eta = sum(p.numel() for p in eta.parameters() if p.requires_grad)  # we count trainable params of eta\n",
    "            print(f\"b network parameters: {n_params_b:,}\")  # we print number of parameters of b\n",
    "            print(f\"eta network parameters: {n_params_eta:,}\")  # we print number of parameters of eta\n",
    "            \n",
    "            # we create optimizers and schedulers\n",
    "            opt_b = torch.optim.Adam(b.parameters(), lr=base_lr)  # we create optimizer for b\n",
    "            opt_eta = torch.optim.Adam(eta.parameters(), lr=base_lr)  # we create optimizer for eta\n",
    "            sched_b = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                opt_b, T_max=n_epochs, eta_min=base_lr * 0.01\n",
    "            )  # we create lr scheduler for b\n",
    "            sched_eta = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                opt_eta, T_max=n_epochs, eta_min=base_lr * 0.01\n",
    "            )  # we create lr scheduler for eta\n",
    "            \n",
    "            # we initialize data dictionary\n",
    "            data_dict = {\n",
    "                \"losses\": [],\n",
    "                \"b_losses\": [],\n",
    "                \"eta_losses\": [],\n",
    "                \"b_grads\": [],\n",
    "                \"eta_grads\": [],\n",
    "                \"lrs\": [],\n",
    "            }  # we store training metrics\n",
    "            \n",
    "            # we train the model for this configuration\n",
    "            print(\"\\nstarting training...\\n\")  # we announce training start\n",
    "            counter = 1  # we initialize iteration counter\n",
    "            for epoch in range(n_epochs):\n",
    "                loss, b_loss, eta_loss, b_grad, eta_grad = train_step(\n",
    "                    batch_size, interpolant, opt_b, opt_eta, sched_b, sched_eta, patch_size, num_patches\n",
    "                )  # we perform one training step\n",
    "                \n",
    "                # we log metrics\n",
    "                if (counter - 1) % metrics_freq == 0:\n",
    "                    data_dict[\"losses\"].append(grab(loss).item())  # we log total loss\n",
    "                    data_dict[\"b_losses\"].append(grab(b_loss).item())  # we log b loss\n",
    "                    data_dict[\"eta_losses\"].append(grab(eta_loss).item())  # we log eta loss\n",
    "                    data_dict[\"b_grads\"].append(grab(b_grad).item())  # we log gradient norm of b\n",
    "                    data_dict[\"eta_grads\"].append(grab(eta_grad).item())  # we log gradient norm of eta\n",
    "                    data_dict[\"lrs\"].append(opt_b.param_groups[0][\"lr\"])  # we log learning rate\n",
    "                    \n",
    "                    print(\n",
    "                        f\"[path={path} | gamma_type={gamma_type}] epoch {counter}: \"\n",
    "                        f\"loss={grab(loss).item():.4f}, \"\n",
    "                        f\"b_loss={grab(b_loss).item():.4f}, \"\n",
    "                        f\"eta_loss={grab(eta_loss).item():.4f}\"\n",
    "                    )  # we print training status\n",
    "                \n",
    "                # we make plots and save checkpoints\n",
    "                if (counter - 1) % plot_freq == 0:\n",
    "                    make_plots(b, eta, interpolant, counter, data_dict, patch_size, num_patches)  # we visualize results\n",
    "                    \n",
    "                    results_dir = os.path.join(\"vresults\", \"cifarpatchedinterpolants\")  # we define directory for checkpoints\n",
    "                    os.makedirs(results_dir, exist_ok=True)  # we ensure results directory exists\n",
    "                    ckpt_name = os.path.join(\n",
    "                        results_dir,\n",
    "                        f\"dog_checkpoint_{path}_{gamma_type}_epoch_{counter}.pt\",\n",
    "                    )  # we build checkpoint filename\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"epoch\": counter,\n",
    "                            \"b_state_dict\": b.state_dict(),\n",
    "                            \"eta_state_dict\": eta.state_dict(),\n",
    "                            \"opt_b_state_dict\": opt_b.state_dict(),\n",
    "                            \"opt_eta_state_dict\": opt_eta.state_dict(),\n",
    "                            \"data_dict\": data_dict,\n",
    "                            \"class\": \"dog\",\n",
    "                            \"class_id\": 5,\n",
    "                            \"path\": path,\n",
    "                            \"gamma_type\": gamma_type,\n",
    "                        },\n",
    "                        ckpt_name,\n",
    "                    )  # we save checkpoint\n",
    "                    print(\n",
    "                        f\"saved checkpoint at epoch {counter} for path={path}, gamma_type={gamma_type}\"\n",
    "                    )  # we log checkpoint saving\n",
    "                \n",
    "                counter += 1  # we increment iteration counter\n",
    "            \n",
    "            print(f\"\\ntraining complete for path={path}, gamma_type={gamma_type}\\n\")  # we mark configuration as finished"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
